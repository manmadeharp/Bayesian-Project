This file is a merged representation of the entire codebase, combining all repository files into a single document.
Generated by Repomix on: 2024-11-13T08:48:16.900Z

================================================================
File Summary
================================================================

Purpose:
--------
This file contains a packed representation of the entire repository's contents.
It is designed to be easily consumable by AI systems for analysis, code review,
or other automated processes.

File Format:
------------
The content is organized as follows:
1. This summary section
2. Repository information
3. Repository structure
4. Multiple file entries, each consisting of:
  a. A separator line (================)
  b. The file path (File: path/to/file)
  c. Another separator line
  d. The full contents of the file
  e. A blank line

Usage Guidelines:
-----------------
- This file should be treated as read-only. Any changes should be made to the
  original repository files, not this packed version.
- When processing this file, use the file path to distinguish
  between different files in the repository.
- Be aware that this file may contain sensitive information. Handle it with
  the same level of security as you would the original repository.

Notes:
------
- Some files may have been excluded based on .gitignore rules and Repomix's
  configuration.
- Binary files are not included in this packed representation. Please refer to
  the Repository Structure section for a complete list of file paths, including
  binary files.

Additional Info:
----------------

For more information about Repomix, visit: https://github.com/yamadashy/repomix

================================================================
Repository Structure
================================================================
BayesianInference/Distributions.py
BayesianInference/MALA.py
BayesianInference/MetropolisHastings.py
BayesianInference/PRNG.py
BayesianInference/repomix-output.txt
MetropolisHastings/BIP_Oscillator.py
ReactionDiffusion.py
Session.vim
TestStubs/MetropolisHastingsTestStub.py

================================================================
Repository Files
================================================================

================
File: BayesianInference/Distributions.py
================
import scipy as sp
import numpy as np
from typing import Tuple, Union
from .PRNG import RNG, SEED


class Proposal:
    def __init__(
        self, proposal_distribution: sp.stats.rv_continuous, scale: np.ndarray
    ):
        self.proposal_distribution = proposal_distribution
        self.proposal = RNG(SEED, proposal_distribution)
        if np.isscalar(scale):
            self.beta = np.sqrt(scale)
        else:
            self.beta = sp.stats.Covariance.from_cholesky(scale)  # L*x ~ N(0, Sigma)

    def propose(self, current: np.ndarray):
        return self.proposal(current, self.beta)

    def proposal_log_density(
        self,
        state: np.ndarray,
        loc: np.ndarray,
    ) -> np.float64:
        return self.proposal_distribution.logpdf(state, loc, self.beta)


# Test Proposal
# test = Proposal(sp.stats.multivariate_normal, np.array([[1, 2], [2, 1]]))
# print(test.propose(np.array([1.0, 12])))
# print(test.proposal_log_density(np.array([1.0, 12]), np.array([1.0, 12])))


class TargetDistribution:
    def __init__(
        self,
        prior: sp.stats.rv_continuous,
        likelihood: sp.stats.rv_continuous,
        data,
        sigma: float,
    ):
        self.prior = prior
        self.likelihood = likelihood
        # likelihood
        self.data = data
        self.data_sigma = sigma

    def log_likelihood(self, x: np.ndarray) -> np.float64:
        """
        Likelihood of our data given the parameters x.
        I.E the distribution of the data given the parameters x.
        :param x:
        :return:
        """
        return np.sum(self.likelihood.logpdf(self.data, x, self.data_sigma))

    def log_prior(self, x: np.ndarray) -> np.float64:
        return self.prior.logpdf(x)


class BayesInverseGammaVarianceDistribution(TargetDistribution):
    """Target distribution with inverse gamma prior on noise variance"""

    def __init__(
            self,
            prior: sp.stats.rv_continuous,
            likelihood: sp.stats.rv_continuous,
            data,
            alpha: float = 2.0,  # Shape parameter for inverse gamma
            beta: float = 1.0,  # Scale parameter for inverse gamma
    ):
        # Initialize parent without sigma since we're marginalizing it
        super().__init__(prior, likelihood, data, sigma=None)

        # Store inverse gamma hyperparameters
        self.alpha = alpha
        self.beta = beta

    def log_likelihood(self, x: np.ndarray) -> np.float64:
        """
        Compute marginalized log likelihood integrating out σ²
        p(y|θ) = ∫ p(y|θ,σ²)p(σ²)dσ²
        """
        residuals = self.data - x  # Or self.forward_model(x) for complex models
        n = len(residuals)
        RSS = np.sum(residuals ** 2)

        # Updated inverse gamma parameters
        alpha_post = self.alpha + n / 2
        beta_post = self.beta + RSS / 2

        # Log marginal likelihood (multivariate t-distribution)
        log_lik = -alpha_post * np.log(beta_post)
        log_lik += sp.special.gammaln(alpha_post) - sp.special.gammaln(self.alpha)
        log_lik -= (n / 2) * np.log(2 * np.pi)

        return np.float64(log_lik)

    def sample_variance_posterior(self, x: np.ndarray) -> float:
        """Sample from conditional posterior of σ² given parameters"""
        residuals = self.data - x
        n = len(residuals)
        RSS = np.sum(residuals ** 2)

        # Posterior inverse gamma parameters
        alpha_post = self.alpha + n / 2
        beta_post = self.beta + RSS / 2

        return sp.stats.invgamma.rvs(alpha_post, scale=beta_post)

================
File: BayesianInference/MALA.py
================
import numpy as np
import scipy as sp
from .MetropolisHastings import MetropolisHastings
from .Distributions import Proposal, TargetDistribution

## Metropolis Adjusted Langevin Algorithm WIP

class GradientComputer:
    """
    Handles computation of gradients for the target distribution.
    Can use either numerical or analytical gradients.
    """
    def __init__(self, target: TargetDistribution, eps: float = 1e-8):
        self.target = target
        self.eps = eps
        
    def numerical_gradient(self, x: np.ndarray) -> np.ndarray:
        """Compute gradient using finite differences"""
        grad = np.zeros_like(x)
        for i in range(len(x)):
            x_plus = x.copy()
            x_plus[i] += self.eps
            x_minus = x.copy()
            x_minus[i] -= self.eps
            
            # Compute gradient of log posterior
            grad[i] = (
                (self.target.log_prior(x_plus) + 
                 self.target.log_likelihood(x_plus) -
                 self.target.log_prior(x_minus) - 
                 self.target.log_likelihood(x_minus)) / (2 * self.eps)
            )
        return grad

class LangevinProposal(Proposal):
    """
    Handles Langevin dynamics-based proposals.
    """
    def __init__(
        self,
        proposal_distribution: sp.stats.rv_continuous,
        step_size: float,
        gradient_computer: GradientComputer
    ):
        super().__init__(proposal_distribution, np.sqrt(step_size))
        self.step_size = step_size
        self.gradient_computer = gradient_computer
        
    def propose(self, current: np.ndarray) -> np.ndarray:
        """Generate proposal using Langevin dynamics"""
        # Compute gradient-based drift
        gradient = self.gradient_computer.numerical_gradient(current)
        mean = current + 0.5 * self.step_size * gradient
        
        # Add noise scaled by sqrt(step_size)
        noise = np.sqrt(self.step_size) * self.proposal(
            np.zeros_like(current),
            np.eye(len(current))
        )
        return mean + noise
        
    def proposal_log_density(self, proposed: np.ndarray, current: np.ndarray) -> np.float64:
        """Compute log density of the Langevin proposal"""
        # Compute means for forward and backward proposals
        forward_gradient = self.gradient_computer.numerical_gradient(current)
        forward_mean = current + 0.5 * self.step_size * forward_gradient
        
        # Use parent class proposal distribution for density computation
        return self.proposal_distribution.logpdf(
            proposed,
            forward_mean,
            np.sqrt(self.step_size) * np.eye(len(current))
        )

class MALA(MetropolisHastings):
    """
    Metropolis-Adjusted Langevin Algorithm
    Uses gradient information for intelligent proposals
    """
    def __init__(
        self,
        target: TargetDistribution,
        step_size: float,
        initial_state: np.ndarray,
    ):
        # Set up gradient computation
        self.gradient_computer = GradientComputer(target)
        
        # Set up Langevin proposal mechanism
        proposal = LangevinProposal(
            sp.stats.multivariate_normal,
            step_size,
            self.gradient_computer
        )
        
        # Initialize parent class
        super().__init__(target, proposal, initial_state)
        
    def acceptance_ratio(self, current: np.ndarray, proposed: np.ndarray) -> np.float64:
        """
        Compute acceptance ratio accounting for asymmetric proposals
        """
        # Standard MH ratio terms
        prior_ratio = (self.target_distribution.log_prior(proposed) - 
                      self.target_distribution.log_prior(current))
        likelihood_ratio = (self.target_distribution.log_likelihood(proposed) - 
                          self.target_distribution.log_likelihood(current))
        
        # Proposal ratio (forward vs backward proposals)
        proposal_ratio = (
            self.proposal_distribution.proposal_log_density(current, proposed) -
            self.proposal_distribution.proposal_log_density(proposed, current)
        )
        
        return min(0.0, prior_ratio + likelihood_ratio + proposal_ratio)

# Example usage with diagnostics
def run_mala_with_diagnostics(
    target: TargetDistribution,
    initial_state: np.ndarray,
    step_size: float,
    n_iterations: int
) -> dict:
    """
    Run MALA algorithm and return diagnostics
    """
    # Initialize and run sampler
    sampler = MALA(target, step_size, initial_state)
    sampler(n_iterations)
    
    # Compute diagnostics
    acceptance_rate = sampler.acceptance_count / n_iterations
    
    return {
        'chain': sampler.chain[:sampler._index],
        'acceptance_rate': acceptance_rate,
        'step_size': step_size
    }

================
File: BayesianInference/MetropolisHastings.py
================
import scipy as sp
import numpy as np
from .PRNG import SEED
from .Distributions import Proposal, TargetDistribution

# Default Values
# - Seed Value


class MetropolisHastings:
    def __init__(
        self,
        target_distribution: TargetDistribution,
        proposal_distribution: Proposal,
        initialstate,
    ):
        """
        Initialize the Metropolis-Hastings algorithm.
        :param target_distribution:
        :param proposal_distribution:
        :param initialstate:
        """
        self.target_distribution = target_distribution
        self.proposal_distribution = proposal_distribution

        self.max_size = 10000  # Or some reasonable default
        self.chain = np.empty((self.max_size, len(initialstate)))
        self.chain[0] = initialstate
        self._index = 1
        self.uniform_rng = np.random.default_rng(
            seed=SEED
        )  # Using Philox for Reproducability

    def acceptance_ratio(self, current: np.ndarray, proposed: np.ndarray) -> np.float64:
        """
        Acceptance ratio for the Metropolis-Hastings algorithm.
        :param current: Current state of the chain.
        :param proposed: Proposed state of the chain.
        :return: Acceptance ratio.
        """

        prior_ratio = self.target_distribution.log_prior(
            proposed
        ) - self.target_distribution.log_prior(current)
        likelihood_ratio = self.target_distribution.log_likelihood(
            proposed
        ) - self.target_distribution.log_likelihood(current)
        transition_ratio = self.proposal_distribution.proposal_log_density(
            current, proposed
        ) - self.proposal_distribution.proposal_log_density(
            proposed, current
        )  # Previous given new over new given previous

        log_ratio = prior_ratio + likelihood_ratio + transition_ratio
        return min(np.float64(0), log_ratio)

    def __call__(self, n: int):
        """
        Run the Metropolis-Hastings algorithm for n iterations.
        :param n:
        """
        # Resize if needed
        if self._index + n > self.max_size:
            new_max = max(self.max_size * 2, self._index + n)
            new_chain = np.empty((new_max, self.chain.shape[1]))
            new_chain[: self._index] = self.chain[: self._index]
            self.chain = new_chain
            self.max_size = new_max

        for i in range(n):
            current = self.chain[self._index - 1]
            proposed = self.proposal_distribution.propose(current)
            if np.log(self.uniform_rng.uniform()) < self.acceptance_ratio(
                current, proposed
            ):
                self.chain[self._index] = proposed
            else:
                self.chain[self._index] = current
            self._index += 1


## WIP
class AdaptiveMetropolisHastings(MetropolisHastings):
    def __init__(
        self,
        target: TargetDistribution,
        proposal: Proposal,
        initial_value,
        adaptation_interval: int = 100,
        target_acceptance: float = 0.234,
        adaptation_scale: float = 2.4,
    ):
        super().__init__(target, proposal, initial_value)
        self.acceptance_count = 0
        self.adaptation_interval = adaptation_interval
        self.target_acceptance = target_acceptance
        self.adaptation_scale = adaptation_scale * 2 / initial_value.size

    def update_proposal(self):
        """Update proposal covariance based on chain history"""
        chain_segment = self.chain[: self._index]

        cov = np.cov(chain_segment.T)
        scaled_cov = (
            self.acceptance_count / self.chain.size
        ) * self.adaptation_scale * cov + np.eye(cov.shape[0]) * 1e-6
        self.proposal_distribution.beta = sp.stats.Covariance.from_cholesky(scaled_cov)

    def __call__(self, n: int):
        """Run adaptive MCMC"""
        # Resize if needed
        if self._index + n > self.max_size:
            new_max = max(self.max_size * 2, self._index + n)
            new_chain = np.empty((new_max, self.chain.shape[1]))
            new_chain[: self._index] = self.chain[: self._index]
            self.chain = new_chain
            self.max_size = new_max

        for i in range(n):
            current = self.chain[self._index - 1]
            proposed = self.proposal_distribution.propose(current)
            if np.log(self.uniform_rng.uniform()) < self.acceptance_ratio(
                current, proposed
            ):
                self.chain[self._index] = proposed
                self.acceptance_count += 1
            else:
                self.chain[self._index] = current

            if (
                self._index % self.adaptation_interval == 0
            ):  # Adapt every 100 iterations
                self.update_proposal()

            self._index += 1

================
File: BayesianInference/PRNG.py
================
from numpy.random import SeedSequence, Generator, Philox
from scipy.stats import rv_continuous
from typing import Optional

# Static Values
SEED = 1000


# Parent RNG class
class RNG:
    def __init__(self, seed: Optional[int], distribution: rv_continuous):
        self.ss = SeedSequence(seed)
        self.rg = Generator(Philox(self.ss))  # Use Philox for parallel applications
        self.rng_distribution = distribution

    def __call__(self, loc, scale, size: Optional[int] = None, *args, **kwargs):
        return self.rng_distribution.rvs(
            loc, scale, size=size, random_state=self.rg, *args, **kwargs
        )

================
File: BayesianInference/repomix-output.txt
================
This file is a merged representation of the entire codebase, combining all repository files into a single document.
Generated by Repomix on: 2024-11-06T09:48:45.846Z

================================================================
File Summary
================================================================

Purpose:
--------
This file contains a packed representation of the entire repository's contents.
It is designed to be easily consumable by AI systems for analysis, code review,
or other automated processes.

File Format:
------------
The content is organized as follows:
1. This summary section
2. Repository information
3. Repository structure
4. Multiple file entries, each consisting of:
  a. A separator line (================)
  b. The file path (File: path/to/file)
  c. Another separator line
  d. The full contents of the file
  e. A blank line

Usage Guidelines:
-----------------
- This file should be treated as read-only. Any changes should be made to the
  original repository files, not this packed version.
- When processing this file, use the file path to distinguish
  between different files in the repository.
- Be aware that this file may contain sensitive information. Handle it with
  the same level of security as you would the original repository.

Notes:
------
- Some files may have been excluded based on .gitignore rules and Repomix's
  configuration.
- Binary files are not included in this packed representation. Please refer to
  the Repository Structure section for a complete list of file paths, including
  binary files.

Additional Info:
----------------

For more information about Repomix, visit: https://github.com/yamadashy/repomix

================================================================
Repository Structure
================================================================
Distributions.py
MALA.py
MetropolisHastings.py
PRNG.py

================================================================
Repository Files
================================================================

================
File: Distributions.py
================
import scipy as sp
import numpy as np
from typing import Tuple, Union
from .PRNG import RNG, SEED


class Proposal:
    def __init__(
        self, proposal_distribution: sp.stats.rv_continuous, scale: np.ndarray
    ):
        self.proposal_distribution = proposal_distribution
        self.proposal = RNG(SEED, proposal_distribution)
        if np.isscalar(scale):
            self.beta = np.sqrt(scale)
        else:
            self.beta = sp.stats.Covariance.from_cholesky(scale)  # L*x ~ N(0, Sigma)

    def propose(self, current: np.ndarray):
        return self.proposal(current, self.beta)

    def proposal_log_density(
        self,
        state: np.ndarray,
        loc: np.ndarray,
    ) -> np.float64:
        return self.proposal_distribution.logpdf(state, loc, self.beta)


# Test Proposal
# test = Proposal(sp.stats.multivariate_normal, np.array([[1, 2], [2, 1]]))
# print(test.propose(np.array([1.0, 12])))
# print(test.proposal_log_density(np.array([1.0, 12]), np.array([1.0, 12])))


class TargetDistribution:
    def __init__(
        self,
        prior: sp.stats.rv_continuous,
        likelihood: sp.stats.rv_continuous,
        data,
        sigma: float,
    ):
        self.prior = prior
        self.likelihood = likelihood
        # likelihood
        self.data = data
        self.data_sigma = sigma

    def log_likelihood(self, x: np.ndarray) -> np.float64:
        """
        Likelihood of our data given the parameters x.
        I.E the distribution of the data given the parameters x.
        :param x:
        :return:
        """
        return np.sum(self.likelihood.logpdf(self.data, x, self.data_sigma))

    def log_prior(self, x: np.ndarray) -> np.float64:
        return self.prior.logpdf(x)

================
File: MALA.py
================
import numpy as np
import scipy as sp
from .MetropolisHastings import MetropolisHastings
from .Distributions import Proposal, TargetDistribution

## Metropolis Adjusted Langevin Algorithm WIP

class GradientComputer:
    """
    Handles computation of gradients for the target distribution.
    Can use either numerical or analytical gradients.
    """
    def __init__(self, target: TargetDistribution, eps: float = 1e-8):
        self.target = target
        self.eps = eps
        
    def numerical_gradient(self, x: np.ndarray) -> np.ndarray:
        """Compute gradient using finite differences"""
        grad = np.zeros_like(x)
        for i in range(len(x)):
            x_plus = x.copy()
            x_plus[i] += self.eps
            x_minus = x.copy()
            x_minus[i] -= self.eps
            
            # Compute gradient of log posterior
            grad[i] = (
                (self.target.log_prior(x_plus) + 
                 self.target.log_likelihood(x_plus) -
                 self.target.log_prior(x_minus) - 
                 self.target.log_likelihood(x_minus)) / (2 * self.eps)
            )
        return grad

class LangevinProposal(Proposal):
    """
    Handles Langevin dynamics-based proposals.
    """
    def __init__(
        self,
        proposal_distribution: sp.stats.rv_continuous,
        step_size: float,
        gradient_computer: GradientComputer
    ):
        super().__init__(proposal_distribution, np.sqrt(step_size))
        self.step_size = step_size
        self.gradient_computer = gradient_computer
        
    def propose(self, current: np.ndarray) -> np.ndarray:
        """Generate proposal using Langevin dynamics"""
        # Compute gradient-based drift
        gradient = self.gradient_computer.numerical_gradient(current)
        mean = current + 0.5 * self.step_size * gradient
        
        # Add noise scaled by sqrt(step_size)
        noise = np.sqrt(self.step_size) * self.proposal(
            np.zeros_like(current),
            np.eye(len(current))
        )
        return mean + noise
        
    def proposal_log_density(self, proposed: np.ndarray, current: np.ndarray) -> np.float64:
        """Compute log density of the Langevin proposal"""
        # Compute means for forward and backward proposals
        forward_gradient = self.gradient_computer.numerical_gradient(current)
        forward_mean = current + 0.5 * self.step_size * forward_gradient
        
        # Use parent class proposal distribution for density computation
        return self.proposal_distribution.logpdf(
            proposed,
            forward_mean,
            np.sqrt(self.step_size) * np.eye(len(current))
        )

class MALA(MetropolisHastings):
    """
    Metropolis-Adjusted Langevin Algorithm
    Uses gradient information for intelligent proposals
    """
    def __init__(
        self,
        target: TargetDistribution,
        step_size: float,
        initial_state: np.ndarray,
    ):
        # Set up gradient computation
        self.gradient_computer = GradientComputer(target)
        
        # Set up Langevin proposal mechanism
        proposal = LangevinProposal(
            sp.stats.multivariate_normal,
            step_size,
            self.gradient_computer
        )
        
        # Initialize parent class
        super().__init__(target, proposal, initial_state)
        
    def acceptance_ratio(self, current: np.ndarray, proposed: np.ndarray) -> np.float64:
        """
        Compute acceptance ratio accounting for asymmetric proposals
        """
        # Standard MH ratio terms
        prior_ratio = (self.target_distribution.log_prior(proposed) - 
                      self.target_distribution.log_prior(current))
        likelihood_ratio = (self.target_distribution.log_likelihood(proposed) - 
                          self.target_distribution.log_likelihood(current))
        
        # Proposal ratio (forward vs backward proposals)
        proposal_ratio = (
            self.proposal_distribution.proposal_log_density(current, proposed) -
            self.proposal_distribution.proposal_log_density(proposed, current)
        )
        
        return min(0.0, prior_ratio + likelihood_ratio + proposal_ratio)

# Example usage with diagnostics
def run_mala_with_diagnostics(
    target: TargetDistribution,
    initial_state: np.ndarray,
    step_size: float,
    n_iterations: int
) -> dict:
    """
    Run MALA algorithm and return diagnostics
    """
    # Initialize and run sampler
    sampler = MALA(target, step_size, initial_state)
    sampler(n_iterations)
    
    # Compute diagnostics
    acceptance_rate = sampler.acceptance_count / n_iterations
    
    return {
        'chain': sampler.chain[:sampler._index],
        'acceptance_rate': acceptance_rate,
        'step_size': step_size
    }

================
File: MetropolisHastings.py
================
import scipy as sp
import numpy as np
from .PRNG import SEED
from .Distributions import Proposal, TargetDistribution

# Default Values
# - Seed Value


class MetropolisHastings:
    def __init__(
        self,
        target_distribution: TargetDistribution,
        proposal_distribution: Proposal,
        initialstate,
    ):
        """
        Initialize the Metropolis-Hastings algorithm.
        :param target_distribution:
        :param proposal_distribution:
        :param initialstate:
        """
        self.target_distribution = target_distribution
        self.proposal_distribution = proposal_distribution

        self.max_size = 10000  # Or some reasonable default
        self.chain = np.empty((self.max_size, len(initialstate)))
        self.chain[0] = initialstate
        self._index = 1
        self.uniform_rng = np.random.default_rng(
            seed=SEED
        )  # Using Philox for Reproducability

    def acceptance_ratio(self, current: np.ndarray, proposed: np.ndarray) -> np.float64:
        """
        Acceptance ratio for the Metropolis-Hastings algorithm.
        :param current: Current state of the chain.
        :param proposed: Proposed state of the chain.
        :return: Acceptance ratio.
        """

        prior_ratio = self.target_distribution.log_prior(
            proposed
        ) - self.target_distribution.log_prior(current)
        likelihood_ratio = self.target_distribution.log_likelihood(
            proposed
        ) - self.target_distribution.log_likelihood(current)
        transition_ratio = self.proposal_distribution.proposal_log_density(
            current, proposed
        ) - self.proposal_distribution.proposal_log_density(
            proposed, current
        )  # Previous given new over new given previous

        log_ratio = prior_ratio + likelihood_ratio + transition_ratio
        return min(np.float64(0), log_ratio)

    def __call__(self, n: int):
        """
        Run the Metropolis-Hastings algorithm for n iterations.
        :param n:
        """
        # Resize if needed
        if self._index + n > self.max_size:
            new_max = max(self.max_size * 2, self._index + n)
            new_chain = np.empty((new_max, self.chain.shape[1]))
            new_chain[: self._index] = self.chain[: self._index]
            self.chain = new_chain
            self.max_size = new_max

        for i in range(n):
            current = self.chain[self._index - 1]
            proposed = self.proposal_distribution.propose(current)
            if np.log(self.uniform_rng.uniform()) < self.acceptance_ratio(
                current, proposed
            ):
                self.chain[self._index] = proposed
            else:
                self.chain[self._index] = current
            self._index += 1


## WIP
class AdaptiveMetropolisHastings(MetropolisHastings):
    def __init__(
        self,
        target: TargetDistribution,
        proposal: Proposal,
        initial_value,
        adaptation_interval: int = 100,
        target_acceptance: float = 0.234,
        adaptation_scale: float = 2.4,
    ):
        super().__init__(target, proposal, initial_value)
        self.acceptance_count = 0
        self.adaptation_interval = adaptation_interval
        self.target_acceptance = target_acceptance
        self.adaptation_scale = adaptation_scale * 2 / initial_value.size

    def update_proposal(self):
        """Update proposal covariance based on chain history"""
        chain_segment = self.chain[: self._index]

        cov = np.cov(chain_segment.T)
        scaled_cov = (
            self.acceptance_count / self.chain.size
        ) * self.adaptation_scale * cov + np.eye(cov.shape[0]) * 1e-6
        self.proposal_distribution.beta = sp.stats.Covariance.from_cholesky(scaled_cov)

    def __call__(self, n: int):
        """Run adaptive MCMC"""
        # Resize if needed
        if self._index + n > self.max_size:
            new_max = max(self.max_size * 2, self._index + n)
            new_chain = np.empty((new_max, self.chain.shape[1]))
            new_chain[: self._index] = self.chain[: self._index]
            self.chain = new_chain
            self.max_size = new_max

        for i in range(n):
            current = self.chain[self._index - 1]
            proposed = self.proposal_distribution.propose(current)
            if np.log(self.uniform_rng.uniform()) < self.acceptance_ratio(
                current, proposed
            ):
                self.chain[self._index] = proposed
                self.acceptance_count += 1
            else:
                self.chain[self._index] = current

            if (
                self._index % self.adaptation_interval == 0
            ):  # Adapt every 100 iterations
                self.update_proposal()

            self._index += 1

================
File: PRNG.py
================
from numpy.random import SeedSequence, Generator, Philox
from scipy.stats import rv_continuous
from typing import Optional

# Static Values
SEED = 1000


# Parent RNG class
class RNG:
    def __init__(self, seed: Optional[int], distribution: rv_continuous):
        self.ss = SeedSequence(seed)
        self.rg = Generator(Philox(self.ss))  # Use Philox for parallel applications
        self.rng_distribution = distribution

    def __call__(self, loc, scale, size: Optional[int] = None, *args, **kwargs):
        return self.rng_distribution.rvs(
            loc, scale, size=size, random_state=self.rg, *args, **kwargs
        )

================
File: MetropolisHastings/BIP_Oscillator.py
================
import matplotlib.pyplot as plt
import numpy as np
import scipy as sp
from Implementations.BayesianInference import SEED
from Implementations.BayesianInference import TargetDistribution, Proposal
from Implementations.BayesianInference.MetropolisHastings import MetropolisHastings

# The static SEED variable for reproducibility
Seed = 1

# True parameter values for damped oscillator
true_params = np.array([0.2, 2.0])  # [damping, frequency]


# Generate synthetic data
def oscillator(t, params):
    """Damped oscillator solution"""
    damping, freq = params
    return np.exp(-damping * t) * np.cos(freq * t)


# Generate data points
t_obs = np.linspace(0, 10, 100)
true_signal = oscillator(t_obs, true_params)
noise_level = 0.1
noise_rng = np.random.Generator(np.random.Philox(np.random.SeedSequence(SEED)))

noisy_data = true_signal + noise_rng.normal(0, noise_level, size=len(t_obs))


class OscillatorTarget(TargetDistribution):
    def __init__(self, data, t, noise_sigma):
        # Use parent's structure as is
        prior = sp.stats.uniform(loc=[0, 0], scale=[1, 5])
        likelihood = sp.stats.norm
        super().__init__(prior, likelihood, data, noise_sigma)
        self.t = t

    def forward_model(self, params: np.ndarray) -> np.ndarray:
        """Transform parameters to predictions"""
        return oscillator(self.t, params)

    # Parent log_likelihood and log_prior stay the same!
    # Just use forward_model when needed:
    def log_likelihood(self, x: np.ndarray) -> np.float64:
        predicted = self.forward_model(x)
        return super().log_likelihood(predicted)

    def log_prior(self, x):
        return np.sum(super().log_prior(x))  # Sum the log probabilities


# Set up MCMC
target = OscillatorTarget(noisy_data, t_obs, noise_level)
proposal = Proposal(
    sp.stats.multivariate_normal, scale=np.array([[0.1, 0], [0, 0.1]])
)  # Small step size

# Initial guess
initial_state = np.array([0.1, 1.5])

# Run MCMC
mcmc = MetropolisHastings(target, proposal, initial_state)
mcmc(5000)

# Plot results
plt.figure(figsize=(12, 4))

# Plot data and fit
plt.subplot(121)
plt.plot(t_obs, noisy_data, "k.", label="Data")
plt.plot(t_obs, true_signal, "g-", label="True")
final_params = mcmc.chain[4000]  # Use a late sample
plt.plot(t_obs, oscillator(t_obs, final_params), "r--", label="Estimated")
plt.legend()
plt.title("Data and Fit")

# Plot parameter traces
plt.subplot(122)
plt.plot(mcmc.chain[:4000, 0], mcmc.chain[:4000, 1], "k.", alpha=0.1)
plt.plot(true_params[0], true_params[1], "r*", markersize=10)
plt.xlabel("Damping")
plt.ylabel("Frequency")
plt.title("Parameter Space")

plt.show()

================
File: ReactionDiffusion.py
================
import numpy as np
import scipy.stats as sp
from scipy.integrate import solve_bvp
import matplotlib.pyplot as plt
import os
import sys

sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

from BayesianInference.PRNG import RNG, SEED
from BayesianInference.Distributions import TargetDistribution, Proposal
from BayesianInference.MetropolisHastings import MetropolisHastings


class ReactionDiffusionTarget(TargetDistribution):
    """Target distribution for the reaction-diffusion inverse problem"""

    def __init__(self, data, x_points, noise_sigma, boundary_conditions):
        # Set up the prior for [r1, r2, D] parameters
        prior = sp.uniform(loc=[0, 0, 0], scale=[5, 5, 1])
        likelihood = sp.t  # Using t-distribution with 6 degrees of freedom
        super().__init__(prior, likelihood, data, noise_sigma)

        self.x = x_points
        self.bc_a, self.bc_b = boundary_conditions

    def log_likelihood(self, x) -> np.float64:
        """Override parent log_likelihood to handle solver failures"""
        predicted = self.forward_model(x)
        if np.any(np.isinf(predicted)):
            return np.float64(-np.inf)
        # Use t-distribution with 6 degrees of freedom
        return np.float64(
            np.sum(
                self.likelihood.logpdf(
                    self.data - predicted,  # residuals
                    df=4,  # degrees of freedom
                    loc=0,  # mean
                    scale=self.data_sigma,  # scale
                )
            )
        )

    def log_prior(self, x: np.ndarray) -> np.float64:
        """Log prior probability"""
        if np.any(x <= 0):  # Parameters must be positive
            return np.float64(-np.inf)
        return np.float64(np.sum(super().log_prior(x)))

    def forward_model(self, params):
        solution = self.solve_steady_state(params)
        if solution is None:
            return np.full_like(self.data, np.inf)
        return solution

    def solve_steady_state(self, params):
        """Solve the steady state reaction-diffusion equation"""
        D = params[2]  # Diffusion coefficient

        def ode_system(x, y):
            # y[0] is u, y[1] is u'
            return np.vstack((y[1], -self.reaction_term(y[0], params) / D))

        def boundary_conditions(ya, yb):
            return np.array([ya[0] - self.bc_a, yb[0] - self.bc_b])

        # Initial guess for solution
        y = np.zeros((2, len(self.x)))
        y[0] = np.linspace(self.bc_a, self.bc_b, len(self.x))

        try:
            sol = solve_bvp(ode_system, boundary_conditions, self.x, y)
            if not sol.success:
                return None
            return sol.sol(self.x)[0]
        except:
            return None

    def reaction_term(self, u, params):
        """Cubic reaction term R(u) = r1*u - r2*u^3"""
        r1, r2, _ = params
        return r1 * u - r2 * u**3


def generate_synthetic_data(x_points, true_params, noise_sigma, boundary_conditions):
    """Generate synthetic data for testing"""
    model = ReactionDiffusionTarget(None, x_points, noise_sigma, boundary_conditions)
    true_solution = model.solve_steady_state(true_params)

    if true_solution is None:
        raise ValueError("Failed to solve ODE for true parameters")

    # Use the configured PRNG from your library
    noise_rng = RNG(SEED, sp.norm)
    noisy_data = true_solution + noise_rng(0, noise_sigma, len(true_solution))
    return noisy_data, true_solution


if __name__ == "__main__":
    # Problem setup
    x_points = np.linspace(0, 1, 100)
    noise_sigma = 0.2
    true_params = np.array([2.0, 1.0, 0.1])  # [r1, r2, D]
    boundary_conditions = (0.2, 0.2)  # Non-zero but moderate boundary conditions

    # Generate synthetic data
    noisy_data, true_solution = generate_synthetic_data(
        x_points, true_params, noise_sigma, boundary_conditions
    )
    print("True solution shape:", true_solution.shape)
    print("True solution range:", np.min(true_solution), np.max(true_solution))

    # Set up MCMC with same boundary conditions
    target = ReactionDiffusionTarget(
        noisy_data, x_points, noise_sigma, boundary_conditions
    )
    scale = np.diag([1, 1, 0.5])
    proposal = Proposal(sp.multivariate_normal, scale=scale)
    initial_state = np.array([1.5, 0.8, 0.08])
    mcmc = MetropolisHastings(target, proposal, initial_state)

    # Run MCMC
    n_iterations = 50000
    mcmc(n_iterations)

    # Debug chain shape
    print("Chain shape:", mcmc.chain.shape)
    print("Active chain shape:", mcmc.chain[: mcmc._index].shape)

    # Plot results
    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 5))

    # Parameter traces with fixed x-axis - only plot actual chain values
    active_chain = mcmc.chain[: mcmc._index]  # Only use actual samples
    for i, (param, label) in enumerate(zip(active_chain.T, ["r₁", "r₂", "D"])):
        ax1.plot(np.arange(len(param)), param, label=label)
    ax1.set_xlabel("Iteration")
    ax1.set_ylabel("Parameter Value")
    ax1.set_title("Parameter Traces")
    ax1.legend()

    # Data and fit comparison
    ax2.plot(x_points, noisy_data, "k.", alpha=0.5, label="Data")
    ax2.plot(x_points, true_solution, "g-", label="True")

    final_params = active_chain[-1]  # Use last actual sample
    print("\nFinal parameters:", final_params)
    final_solution = target.solve_steady_state(final_params)
    print("Final solution exists:", final_solution is not None)
    if final_solution is not None:
        print("Final solution range:", np.min(final_solution), np.max(final_solution))
        ax2.plot(x_points, final_solution, "r--", label="MCMC Fit")

    ax2.set_xlabel("x")
    ax2.set_ylabel("u(x)")
    ax2.set_title("Data and Model Fit")
    ax2.legend()

    plt.tight_layout()
    plt.show()

    # Print results using chain values
    print("\nResults:")
    print("True parameters:", true_params)
    print("Estimated parameters (mean of last 100 samples):")
    print(np.mean(active_chain[-100:], axis=0))
    print("\nParameter standard deviations:")
    print(np.std(active_chain[-100:], axis=0))

    plt.hist(
        mcmc.chain[: mcmc._index],
        bins=30,
        density=True,
        histtype="step",
        label=["r1", "r2", "D"],
    )
    plt.title("Parameter Distribution after Sampling")
    plt.xlabel("Parameter Value")
    plt.legend()
    plt.show()

================
File: Session.vim
================
let SessionLoad = 1
let s:so_save = &g:so | let s:siso_save = &g:siso | setg so=0 siso=0 | setl so=-1 siso=-1
let v:this_session=expand("<sfile>:p")
silent only
silent tabonly
cd ~/source
if expand('%') == '' && !&modified && line('$') <= 1 && getline(1) == ''
  let s:wipebuf = bufnr('%')
endif
let s:shortmess_save = &shortmess
if &shortmess =~ 'A'
  set shortmess=aoOA
else
  set shortmess=aoO
endif
badd +1 Bayesian-Project
badd +1 Bayesian-Project/Random_Walk_Metropolis_Hastings.ipynb
badd +2 ~/PycharmProjects/Bayesian-Project/rwh.py
badd +78 ~/AppData/Local/nvim/init.lua
badd +28 term://~/source//17460:C:/Windows/System32/WindowsPowerShell/v1.0/powershell.exe
badd +1 term://~/source//23388:.
badd +193 ~/PycharmProjects/Bayesian-Project/rwh_univariable.py
badd +1 term://~/source//20792:.
badd +3761 term://~/source//16940:C:/Windows/System32/WindowsPowerShell/v1.0/powershell.exe
badd +585 term://~/source//16884:C:/Windows/System32/WindowsPowerShell/v1.0/powershell.exe
badd +374 term://~/source//6708:C:/Windows/System32/WindowsPowerShell/v1.0/powershell.exe
badd +1 ./Implementations/RandomWalkMetropolisHastings.py
badd +10 ~/PycharmProjects/Bayesian-Project/Implementations/RandomWalkMetropolisHastings.py
badd +3949 term://~/source//6576:C:/Windows/System32/WindowsPowerShell/v1.0/powershell.exe
badd +5 ~/PycharmProjects/Bayesian-Project/Implementations/MetropolisHastings/RandomWalkMetropolisHastings.py
badd +44 ~/PycharmProjects/Bayesian-Project/Implementations/MetropolisHastings/Distributions.py
badd +43 term://~/PycharmProjects/Bayesian-Project/Implementations//4708:C:/Windows/System32/WindowsPowerShell/v1.0/powershell.exe
badd +1 ~/AppData/Local/nvim/lua/plugins.lua
argglobal
%argdel
$argadd Bayesian-Project
edit ~/AppData/Local/nvim/lua/plugins.lua
argglobal
balt ~/AppData/Local/nvim/init.lua
setlocal fdm=manual
setlocal fde=0
setlocal fmr={{{,}}}
setlocal fdi=#
setlocal fdl=0
setlocal fml=1
setlocal fdn=20
setlocal fen
silent! normal! zE
let &fdl = &fdl
let s:l = 1 - ((0 * winheight(0) + 34) / 69)
if s:l < 1 | let s:l = 1 | endif
keepjumps exe s:l
normal! zt
keepjumps 1
normal! 0
lcd ~/PycharmProjects/Bayesian-Project/Implementations
tabnext 1
if exists('s:wipebuf') && len(win_findbuf(s:wipebuf)) == 0 && getbufvar(s:wipebuf, '&buftype') isnot# 'terminal'
  silent exe 'bwipe ' . s:wipebuf
endif
unlet! s:wipebuf
set winheight=1 winwidth=20
let &shortmess = s:shortmess_save
let s:sx = expand("<sfile>:p:r")."x.vim"
if filereadable(s:sx)
  exe "source " . fnameescape(s:sx)
endif
let &g:so = s:so_save | let &g:siso = s:siso_save
set hlsearch
doautoall SessionLoadPost
unlet SessionLoad
" vim: set ft=vim :

================
File: TestStubs/MetropolisHastingsTestStub.py
================
import sys
import os

import numpy as np
import scipy.stats as stats
import unittest
sys.path.append(os.path.abspath(os.path.join(os.path.dirname(__file__), '../MetropolisHastings')))
from MetropolisHastings import RNG, Proposal, TargetDistribution, MetropolisHastings  # Import your classes here


class TestRNG(unittest.TestCase):
    def setUp(self):
        self.seed = 42
        self.n = 5
        self.distribution = stats.norm
        self.rng = RNG(self.seed, self.n, self.distribution)

    def test_rng_generation(self):
        loc = 0
        scale = 1
        samples = self.rng(loc=loc, scale=scale)
        self.assertEqual(samples.shape, (self.n,))
        self.assertTrue(np.all(np.isfinite(samples)))


class TestProposal(unittest.TestCase):
    def setUp(self):
        self.seed = 42
        self.parameter = np.array([0.0])
        self.proposal_distribution = stats.norm
        self.proposal = Proposal(self.proposal_distribution, self.parameter)

    def test_propose(self):
        current = np.array([1.0])
        proposal_samples = self.proposal.propose(current)
        self.assertEqual(proposal_samples.shape, (self.proposal.proposal_dimension,))
        self.assertTrue(np.all(np.isfinite(proposal_samples)))


class TestTargetDistribution(unittest.TestCase):
    def setUp(self):
        self.prior = stats.norm(loc=0, scale=1)
        self.likelihood = stats.norm(loc=1, scale=1)
        self.data = 1.5
        self.sigma = 0.5
        self.target_dist = TargetDistribution(self.prior, self.likelihood, self.data, self.sigma)

    def test_log_likelihood(self):
        x = np.array([1.0])
        log_likelihood = self.target_dist.log_likelihood(x)
        self.assertTrue(np.isfinite(log_likelihood))

    def test_log_prob(self):
        x = np.array([1.0])
        log_prob = self.target_dist.log_prob(x)
        self.assertTrue(np.isfinite(log_prob))


class TestMetropolisHastings(unittest.TestCase):
    def setUp(self):
        self.target_dist = TargetDistribution(
            prior=stats.norm(loc=0, scale=1),
            likelihood=stats.norm(loc=1, scale=1),
            data=1.5,
            sigma=0.5
        )
        self.proposal_dist = stats.norm(loc=0, scale=1)
        self.initial_state = np.array([0.0])
        self.mh = MetropolisHastings(self.target_dist, self.proposal_dist, self.initial_state)

    def test_initialization(self):
        self.assertTrue(np.array_equal(self.mh.initial_state, self.initial_state))
        self.assertEqual(self.mh.chain.shape, (1, 1))  # Initial state shape should be (1, dimension)

    def test_prior(self):
        # Implement a simple test for the prior method
        # Note: This method currently has no implementation; this is just a placeholder
        pass


if __name__ == "__main__":
    unittest.main()
