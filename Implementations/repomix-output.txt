This file is a merged representation of the entire codebase, combining all repository files into a single document.
Generated by Repomix on: 2025-01-06T12:57:48.378Z

================================================================
File Summary
================================================================

Purpose:
--------
This file contains a packed representation of the entire repository's contents.
It is designed to be easily consumable by AI systems for analysis, code review,
or other automated processes.

File Format:
------------
The content is organized as follows:
1. This summary section
2. Repository information
3. Directory structure
4. Multiple file entries, each consisting of:
  a. A separator line (================)
  b. The file path (File: path/to/file)
  c. Another separator line
  d. The full contents of the file
  e. A blank line

Usage Guidelines:
-----------------
- This file should be treated as read-only. Any changes should be made to the
  original repository files, not this packed version.
- When processing this file, use the file path to distinguish
  between different files in the repository.
- Be aware that this file may contain sensitive information. Handle it with
  the same level of security as you would the original repository.

Notes:
------
- Some files may have been excluded based on .gitignore rules and Repomix's
  configuration.
- Binary files are not included in this packed representation. Please refer to
  the Repository Structure section for a complete list of file paths, including
  binary files.

Additional Info:
----------------

================================================================
Directory Structure
================================================================
BayesianInference/BayesianInference.egg-info/dependency_links.txt
BayesianInference/BayesianInference.egg-info/PKG-INFO
BayesianInference/BayesianInference.egg-info/SOURCES.txt
BayesianInference/BayesianInference.egg-info/top_level.txt
BayesianInference/Diagnostics.py
BayesianInference/Distributions.py
BayesianInference/MALA.py
BayesianInference/MetropolisHastings.py
BayesianInference/PRNG.py
BayesianInference/setup.py
Experiments/Gaussian_Distribution_Inverse.py
Experiments/Gaussian_Mixtures.py
HeatDiffusion/HeatDiffusion1D.py
HeatDiffusion/HeatDiffusion1DTest.py
MetropolisHastings/BIP_Oscillator.py
ReactionDiffusion.py
README.md
TestStubs/MetropolisHastingsTestStub.py

================================================================
Files
================================================================

================
File: BayesianInference/BayesianInference.egg-info/dependency_links.txt
================


================
File: BayesianInference/BayesianInference.egg-info/PKG-INFO
================
Metadata-Version: 2.1
Name: BayesianInference
Version: 0.1

================
File: BayesianInference/BayesianInference.egg-info/SOURCES.txt
================
setup.py
BayesianInference.egg-info/PKG-INFO
BayesianInference.egg-info/SOURCES.txt
BayesianInference.egg-info/dependency_links.txt
BayesianInference.egg-info/top_level.txt

================
File: BayesianInference/BayesianInference.egg-info/top_level.txt
================


================
File: BayesianInference/Diagnostics.py
================
from typing import Dict, Optional, Tuple

import matplotlib.pyplot as plt
import numpy as np
import scipy as sp
from scipy.stats import gaussian_kde

from BayesianInference.MetropolisHastings import MetropolisHastings


class MCMCDiagnostics:
    """Comprehensive MCMC diagnostics and visualization"""

    def __init__(self, sampler: MetropolisHastings, true_value):
        self.sampler = sampler
        self.chain = sampler.chain[: sampler._index]
        self.n_samples = sampler._index
        self.n_params = self.chain.shape[1]
        self.diagnostics = self._calculate_diagnostics()

    # Core statistical computations
    def _compute_ess(self, x: np.ndarray) -> float:
        """Compute effective sample size for a parameter chain"""
        n = len(x)
        if n <= 1:
            return 0
        acf = np.correlate(x - np.mean(x), x - np.mean(x), mode="full")[n - 1 :]
        acf = acf / acf[0]
        cutoff = np.where((acf < 0) | (acf < 0.05))[0]
        cutoff = cutoff[0] if len(cutoff) > 0 else len(acf)
        ess = n / (1 + 2 * np.sum(acf[1:cutoff]))
        return max(1, ess)

    def _compute_act(self, x: np.ndarray) -> float:
        """Compute integrated autocorrelation time"""
        ess = self._compute_ess(x)
        return len(x) / ess if ess > 0 else np.inf

    def _compute_autocorr(
        self, param_idx: int, max_lag: Optional[int] = None
    ) -> np.ndarray:
        """Compute autocorrelation for a parameter"""
        if max_lag is None:
            max_lag = min(100, self.n_samples // 5)

        acf = []
        x = self.chain[:, param_idx]
        for k in range(max_lag):
            if len(x[k:]) > 0 and len(x[:-k]) > 0:
                correlation = np.corrcoef(x[:-k], x[k:])[0, 1]
                acf.append(correlation)
        return np.array(acf)

    def _calculate_diagnostics(self) -> Dict:
        """Calculate all MCMC diagnostics"""
        # Basic statistics
        mean_estimate = np.mean(self.chain, axis=0)
        cov_estimate = np.cov(self.chain.T)

        # Advanced statistics
        ess = np.array(
            [self._compute_ess(self.chain[:, i]) for i in range(self.n_params)]
        )
        act = np.array(
            [self._compute_act(self.chain[:, i]) for i in range(self.n_params)]
        )

        # R-hat calculation
        n_split = self.n_samples // 2
        chains = [self.chain[:n_split], self.chain[n_split:]]
        within_var = np.mean([np.var(c, axis=0) for c in chains], axis=0)
        chain_means = np.array([np.mean(c, axis=0) for c in chains])
        between_var = np.var(chain_means, axis=0) * n_split
        r_hat = np.sqrt((within_var + between_var / n_split) / within_var)

        return {
            "mean_estimate": mean_estimate,
            "covariance_estimate": cov_estimate,
            "ess": ess,
            "act": act,
            "r_hat": r_hat,
            "acceptance_rate": self.sampler.acceptance_count / self.n_samples,
            "n_samples": self.n_samples,
        }

    # Plotting methods
    def _plot_trajectory(self, ax: plt.Axes):
        """Plot chain trajectory"""
        ax.plot(self.chain[:, 0], self.chain[:, 1], "k.", alpha=0.1, markersize=1)
        ax.set_xlabel("X1")
        ax.set_ylabel("X2")
        ax.set_title("Chain Trajectory")

    def _plot_traces(self, ax: plt.Axes):
        """Plot trace plots with running means"""
        for i in range(self.n_params):
            ax.plot(self.chain[:, i], label=f"X{i+1}", alpha=0.5)
            running_mean = np.cumsum(self.chain[:, i]) / np.arange(
                1, self.n_samples + 1
            )
            ax.plot(running_mean, "--", label=f"Mean X{i+1}")
        ax.set_xlabel("Iteration")
        ax.set_title("Trace Plots")
        ax.legend()

    def _plot_autocorr(self, ax: plt.Axes):
        """Plot autocorrelation"""
        max_lag = min(100, self.n_samples // 5)
        for i in range(self.n_params):
            acf = self._compute_autocorr(i, max_lag)
            ax.plot(acf, label=f"X{i+1}")
        ax.set_title("Autocorrelation")
        ax.legend()

    def _plot_acceptance_rate(self, ax: plt.Axes):
        """Plot running acceptance rate"""
        #        window = min(500, self.n_samples // 20)
        #        acc_rates = [np.mean(np.diff(self.chain[i:i+window, 0]) != 0)
        #                    for i in range(0, self.n_samples-window, window)]
        #        ax.plot(range(0, self.n_samples-window, window), acc_rates)
        ax.plot(
            range(0, self.n_samples), self.sampler.acceptance_rates[: self.n_samples]
        )
        ax.axhline(y=0.234, color="r", linestyle="--", label="Optimal")
        ax.set_xlabel("Iteration")
        ax.set_ylabel("Acceptance Rate")
        ax.set_title("Running Acceptance Rate")
        ax.legend()

    def _plot_ess(self, ax: plt.Axes):
        """Plot effective sample size"""
        ax.bar([f"X{i+1}" for i in range(self.n_params)], self.diagnostics["ess"])
        ax.set_ylabel("ESS")
        ax.set_title("Effective Sample Size")

    def _plot_marginals(self, ax: plt.Axes):
        """Plot marginal distributions"""
        for i in range(self.n_params):
            ax.hist(self.chain[:, i], bins=50, density=True, alpha=0.5, label=f"X{i+1}")
        ax.set_title("Marginal Distributions")
        ax.legend()

    # Main public methods
    def plot_diagnostics(self, show: bool = True) -> Tuple[plt.Figure, np.ndarray]:
        """Create comprehensive diagnostic plots"""
        fig, axes = plt.subplots(2, 3, figsize=(15, 10))

        self._plot_trajectory(axes[0, 0])
        self._plot_traces(axes[0, 1])
        self._plot_autocorr(axes[0, 2])
        self._plot_acceptance_rate(axes[1, 0])
        self._plot_ess(axes[1, 1])
        self._plot_marginals(axes[1, 2])

        plt.tight_layout()
        if show:
            plt.show()
        return fig, axes

    def print_summary(self):
        """Print comprehensive MCMC diagnostics summary"""
        print("\nMCMC Summary Statistics:")
        print("=" * 50)

        print("\nConvergence Diagnostics:")
        print(f"R-hat: {self.diagnostics['r_hat']}")
        print(f"Effective Sample Size: {self.diagnostics['ess']}")
        print(f"Integrated ACT: {self.diagnostics['act']}")

        print("\nSampling Statistics:")
        print(f"Final acceptance rate: {self.diagnostics['acceptance_rate']:.3f}")
        print(f"Number of samples: {self.diagnostics['n_samples']}")

        print("\nParameter Estimates:")
        print("Means:")
        print(f"Estimated: {self.diagnostics['mean_estimate']}")
        print("\nCovariance:")
        print(self.diagnostics["covariance_estimate"])

    def plot_target_distribution(self):
        """Plot an approximation of the target distribution with KDE and MCMC samples"""
        # Extract chain samples
        x_samples, y_samples = self.chain[:, 0], self.chain[:, 1]

        # Compute KDE on the samples
        kde = gaussian_kde(self.chain.T)

        # Determine plot ranges with padding
        x_min, x_max = x_samples.min(), x_samples.max()
        y_min, y_max = y_samples.min(), y_samples.max()
        x_pad = 0.1 * (x_max - x_min)
        y_pad = 0.1 * (y_max - y_min)

        # Create grid for plotting
        x = np.linspace(x_min - x_pad, x_max + x_pad, 500)
        y = np.linspace(y_min - y_pad, y_max + y_pad, 500)
        x1, x2 = np.meshgrid(x, y)
        pos = np.vstack([x1.ravel(), x2.ravel()])

        # Evaluate KDE on the grid
        Z = kde(pos).reshape(x1.shape)

        # Plot KDE contours
        plt.figure(figsize=(10, 8))
        plt.contourf(x1, x2, Z, levels=30, cmap="viridis", alpha=0.8)
        plt.colorbar(label="Density")

        # Overlay MCMC samples
        plt.scatter(
            x_samples, y_samples, alpha=0.1, color="k", s=1, label="MCMC samples"
        )

        ax = plt.gca()
        ax.set_aspect("equal", adjustable="box")

        plt.xlabel("X1")
        plt.ylabel("X2")
        plt.title("Target Distribution Approximation with MCMC Samples")
        plt.legend()
        plt.show()

================
File: BayesianInference/Distributions.py
================
from typing import Tuple, Union

import numpy as np
import scipy as sp

from .PRNG import RNG, SEED

## TODO: I need to Add Adaptation MCMC Diagnostics As well.
#    # Plot 5: Adaptation factor
#    adapt_factors = []
#   for i in range(len(chain)):
#       sampler._index = i
#       adapt_factors.append(sampler.get_adaptation_weight())
#   axes[1, 1].plot(adapt_factors)
#   axes[1, 1].set_xlabel("Iteration")
#   axes[1, 1].set_ylabel("Adaptation Weight")
#   axes[1, 1].set_title("Adaptation Weight Decay")

class Proposal:
    def __init__(self, proposal_distribution: sp.stats.rv_continuous, scale):
        self.proposal_distribution = proposal_distribution
        self.proposal = RNG(SEED, proposal_distribution)
        if np.isscalar(scale):
            print("scalar")
            self.beta = np.sqrt(scale)
        else:
            self.beta = scale
#            print("Cholesky")
#            self.beta = sp.stats.Covariance.from_cholesky(scale)  # L*x ~ N(0, Sigma)

    def propose(self, current: np.ndarray):
        return self.proposal(current, self.beta)

    def proposal_log_density(
        self,
        state: np.ndarray,
        loc: np.ndarray,
    ) -> np.float64:
        return self.proposal_distribution.logpdf(state, loc, self.beta)


# Test Proposal
# test = Proposal(sp.stats.multivariate_normal, np.array([[1, 2], [2, 1]]))
# print(test.propose(np.array([1.0, 12])))
# print(test.proposal_log_density(np.array([1.0, 12]), np.array([1.0, 12])))


class TargetDistribution:
    def __init__(
        self,
        prior: sp.stats.rv_continuous,
        likelihood: sp.stats.rv_continuous,
        data,
        sigma: float,
    ):
        self.prior = prior
        self.likelihood = likelihood
        # likelihood
        self.data = data
        self.data_sigma = sigma

    def log_likelihood(self, x: np.ndarray) -> np.float64:
        """
        Likelihood of our data given the parameters x.
        I.E the distribution of the data given the parameters x.
        :param x:
        :return:
        """
        return np.sum(self.likelihood.logpdf(self.data, x, self.data_sigma))

    def log_prior(self, x: np.ndarray) -> np.float64:
        if x.ndim == 2 and x.shape[1] == 1:
            x = x.reshape(-1, 1)

        if not hasattr(self.prior, 'mean'):  # Quick way to check if it's non-frozen
            return self.prior.logpdf(x[np.newaxis, :])

        return (self.prior.logpdf(x))


class BayesInverseGammaVarianceDistribution(TargetDistribution):
    """Target distribution with inverse gamma prior on noise variance"""

    def __init__(
        self,
        prior: sp.stats.rv_continuous,
        likelihood: sp.stats.rv_continuous,
        data,
        alpha: float = 2.0,  # Shape parameter for inverse gamma
        beta: float = 1.0,  # Scale parameter for inverse gamma
    ):
        # Initialize parent without sigma since we're marginalizing it
        super().__init__(prior, likelihood, data, sigma=None)

        # Store inverse gamma hyperparameters
        self.alpha = alpha
        self.beta = beta

    def log_likelihood(self, x: np.ndarray) -> np.float64:
        """
        Compute marginalized log likelihood integrating out σ²
        p(y|θ) = ∫ p(y|θ,σ²)p(σ²)dσ²
        """
        residuals = self.data - x  # Or self.forward_model(x) for complex models
        n = len(residuals)
        RSS = np.sum(residuals**2)

        # Updated inverse gamma parameters
        alpha_post = self.alpha + n / 2
        beta_post = self.beta + RSS / 2

        # Log marginal likelihood (multivariate t-distribution)
        log_lik = -alpha_post * np.log(beta_post)
        log_lik += sp.special.gammaln(alpha_post) - sp.special.gammaln(self.alpha)
        log_lik -= (n / 2) * np.log(2 * np.pi)

        return np.float64(log_lik)

    def sample_variance_posterior(self, x: np.ndarray) -> float:
        """Sample from conditional posterior of σ² given parameters"""
        residuals = self.data - x
        n = len(residuals)
        RSS = np.sum(residuals**2)

        # Posterior inverse gamma parameters
        alpha_post = self.alpha + n / 2
        beta_post = self.beta + RSS / 2

        return sp.stats.invgamma.rvs(alpha_post, scale=beta_post)

================
File: BayesianInference/MALA.py
================
import numpy as np
import scipy as sp
from .MetropolisHastings import MetropolisHastings
from .Distributions import Proposal, TargetDistribution

## Metropolis Adjusted Langevin Algorithm WIP

class GradientComputer:
    """
    Handles computation of gradients for the target distribution.
    Can use either numerical or analytical gradients.
    """
    def __init__(self, target: TargetDistribution, eps: float = 1e-8):
        self.target = target
        self.eps = eps
        
    def numerical_gradient(self, x: np.ndarray) -> np.ndarray:
        """Compute gradient using finite differences"""
        grad = np.zeros_like(x)
        for i in range(len(x)):
            x_plus = x.copy()
            x_plus[i] += self.eps
            x_minus = x.copy()
            x_minus[i] -= self.eps
            
            # Compute gradient of log posterior
            grad[i] = (
                (self.target.log_prior(x_plus) + 
                 self.target.log_likelihood(x_plus) -
                 self.target.log_prior(x_minus) - 
                 self.target.log_likelihood(x_minus)) / (2 * self.eps)
            )
        return grad

class LangevinProposal(Proposal):
    """
    Handles Langevin dynamics-based proposals.
    """
    def __init__(
        self,
        proposal_distribution: sp.stats.rv_continuous,
        step_size: float,
        gradient_computer: GradientComputer
    ):
        super().__init__(proposal_distribution, np.sqrt(step_size))
        self.step_size = step_size
        self.gradient_computer = gradient_computer
        
    def propose(self, current: np.ndarray) -> np.ndarray:
        """Generate proposal using Langevin dynamics"""
        # Compute gradient-based drift
        gradient = self.gradient_computer.numerical_gradient(current)
        mean = current + 0.5 * self.step_size * gradient
        
        # Add noise scaled by sqrt(step_size)
        noise = np.sqrt(self.step_size) * self.proposal(
            np.zeros_like(current),
            np.eye(len(current))
        )
        return mean + noise
        
    def proposal_log_density(self, proposed: np.ndarray, current: np.ndarray) -> np.float64:
        """Compute log density of the Langevin proposal"""
        # Compute means for forward and backward proposals
        forward_gradient = self.gradient_computer.numerical_gradient(current)
        forward_mean = current + 0.5 * self.step_size * forward_gradient
        
        # Use parent class proposal distribution for density computation
        return self.proposal_distribution.logpdf(
            proposed,
            forward_mean,
            np.sqrt(self.step_size) * np.eye(len(current))
        )

class MALA(MetropolisHastings):
    """
    Metropolis-Adjusted Langevin Algorithm
    Uses gradient information for intelligent proposals
    """
    def __init__(
        self,
        target: TargetDistribution,
        step_size: float,
        initial_state: np.ndarray,
    ):
        # Set up gradient computation
        self.gradient_computer = GradientComputer(target)
        
        # Set up Langevin proposal mechanism
        proposal = LangevinProposal(
            sp.stats.multivariate_normal,
            step_size,
            self.gradient_computer
        )
        
        # Initialize parent class
        super().__init__(target, proposal, initial_state)
        
    def acceptance_ratio(self, current: np.ndarray, proposed: np.ndarray) -> np.float64:
        """
        Compute acceptance ratio accounting for asymmetric proposals
        """
        # Standard MH ratio terms
        prior_ratio = (self.target_distribution.log_prior(proposed) - 
                      self.target_distribution.log_prior(current))
        likelihood_ratio = (self.target_distribution.log_likelihood(proposed) - 
                          self.target_distribution.log_likelihood(current))
        
        # Proposal ratio (forward vs backward proposals)
        proposal_ratio = (
            self.proposal_distribution.proposal_log_density(current, proposed) -
            self.proposal_distribution.proposal_log_density(proposed, current)
        )
        
        return min(0.0, prior_ratio + likelihood_ratio + proposal_ratio)

# Example usage with diagnostics
def run_mala_with_diagnostics(
    target: TargetDistribution,
    initial_state: np.ndarray,
    step_size: float,
    n_iterations: int
) -> dict:
    """
    Run MALA algorithm and return diagnostics
    """
    # Initialize and run sampler
    sampler = MALA(target, step_size, initial_state)
    sampler(n_iterations)
    
    # Compute diagnostics
    acceptance_rate = sampler.acceptance_count / n_iterations
    
    return {
        'chain': sampler.chain[:sampler._index],
        'acceptance_rate': acceptance_rate,
        'step_size': step_size
    }

================
File: BayesianInference/MetropolisHastings.py
================
import numpy as np
import scipy as sp

from .Distributions import Proposal, TargetDistribution
from .PRNG import SEED

# Default Values
# - Seed Value
# SEED = 112


class MetropolisHastings:
    def __init__(
        self,
        target_distribution: TargetDistribution,
        proposal_distribution: Proposal,
        initialstate,
    ):
        """
        Initialize the Metropolis-Hastings algorithm.
        :param target_distribution:
        :param proposal_distribution:
        :param initialstate:
        """
        self.target_distribution = target_distribution
        self.proposal_distribution = proposal_distribution

        self.max_size = 10000  # Or some reasonable default
        self.chain = np.zeros((self.max_size, len(initialstate)))
        self.chain[0] = initialstate
        self._index = 1
        self.uniform_rng = np.random.default_rng(
            seed=SEED
        )  # Using Philox for Reproducability

        self.acceptance_count = 0
        self.acceptance_rates = np.zeros(self.max_size)

        self.burnt = False

    def acceptance_ratio(self, current: np.ndarray, proposed: np.ndarray) -> np.float64:
        """
        Acceptance ratio for the Metropolis-Hastings algorithm.
        :param current: Current state of the chain.
        :param proposed: Proposed state of the chain.
        :return: Acceptance ratio.
        """

        prior_ratio = self.target_distribution.log_prior(
            proposed
        ) - self.target_distribution.log_prior(current)
        likelihood_ratio = self.target_distribution.log_likelihood(
            proposed
        ) - self.target_distribution.log_likelihood(current)
        transition_ratio = self.proposal_distribution.proposal_log_density(
            current, proposed
        ) - self.proposal_distribution.proposal_log_density(
            proposed, current
        )  # Previous given new over new given previous
        assert np.isscalar(prior_ratio)
        assert np.isscalar(likelihood_ratio)
        assert np.isscalar(transition_ratio)

        log_ratio = prior_ratio + likelihood_ratio + transition_ratio
        return min(np.float64(0), log_ratio)

    def __call__(self, n: int):
        """
        Run the Metropolis-Hastings algorithm for n iterations.
        :param n:
        """
        # Resize if needed
        if self._index + n > self.max_size:
            new_max = max(self.max_size * 2, self._index + n)
            new_chain = np.empty((new_max, self.chain.shape[1]))
            new_chain[: self._index] = self.chain[: self._index]
            self.chain = new_chain
            self.max_size = new_max
            new_acceptance_rates = np.empty(new_max)
            new_acceptance_rates[: self._index] = self.acceptance_rates[: self._index]
            self.acceptance_rates = new_acceptance_rates

        for i in range(n):
            if i == 200:
                self.burn(199)
            current = self.chain[self._index - 1]
            proposed = self.proposal_distribution.propose(current)
            if np.log(self.uniform_rng.uniform()) < self.acceptance_ratio(
                current, proposed
            ):
                self.chain[self._index] = proposed
                self.acceptance_count += 1
            else:
                self.chain[self._index] = current

            self.acceptance_rates[self._index] = self.acceptance_count / self._index
            self._index += 1

    def burn(self, n: int):
        if n < self._index:
            # Keep only samples after burn point
            burned_chain = self.chain[n : self._index]

            # Create new arrays and copy burned data
            new_chain = np.empty((self.max_size, self.chain.shape[1]))
            new_chain[: len(burned_chain)] = burned_chain

            # Reset acceptance tracking
            self.acceptance_count = 0
            self.acceptance_rates = np.zeros(self.max_size)

            # Update chain and index
            self.chain = new_chain
            self._index = len(burned_chain)
            print("burnt is: ", self.burnt)
            self.burnt = True

        else:
            raise ValueError("Burn-in exceeds the number of samples in the chain.")

    #    def burn(self, n):
    #        print("chain delte: ", len(self.chain))
    #        self.chain = self.chain[n:]
    #       print("chain deleteafter: ", len(self.chain))

    def writeToFile(self, filename: str) -> None:
        """
        Write the MCMC chain to a CSV file.

        Args:
            filename: Path to the CSV file
        """
        import numpy as np

        # Get the actual chain data (exclude empty rows)
        chain_data = self.chain[: self._index]

        # Write using numpy's savetxt
        np.savetxt(filename, chain_data, delimiter=",", fmt="%.8f")


class AdaptiveMetropolisHastingsVOne(MetropolisHastings):
    def __init__(
        self,
        target: TargetDistribution,
        proposal: Proposal,
        initial_value: np.ndarray,
        adaptation_interval: int = 50,
        min_samples_adapt: int = 500,
        max_samples_adapt: int = 1000,
    ):
        # Use parent class initialization
        super().__init__(target, proposal, initial_value)

        # Add only what's needed for adaptation
        self.adaptation_interval = adaptation_interval
        self.min_samples_adapt = min_samples_adapt
        self.max_samples_adapt = max_samples_adapt

        # Setup for covariance adaptation
        d = len(initial_value)
        self.scale = (2.38**2) / d
        self.all_samples = np.zeros((max_samples_adapt, d))
        self.all_samples[0] = initial_value
        self.n_samples = 1

    def update_covariance(self, state: np.ndarray):
        """Update covariance estimate with new state"""
        if self.n_samples >= self.max_samples_adapt:
            return

        self.all_samples[self.n_samples] = state

        if self.n_samples > 1:
            # Update proposal covariance using all samples
            sample_cov = np.cov(self.all_samples[: self.n_samples + 1].T)
            reg = 1e-6 * np.diag(np.diag(sample_cov))
            self.proposal_distribution.beta = self.scale * (sample_cov + reg)

        self.n_samples += 1

    def __call__(self, n: int):
        # Use parent's chain management
        if self._index + n > self.max_size:
            new_max = max(self.max_size * 2, self._index + n)
            new_chain = np.empty((new_max, self.chain.shape[1]))
            new_chain[: self._index] = self.chain[: self._index]
            self.chain = new_chain
            new_acceptance_rates = np.empty(new_max)
            new_acceptance_rates[: self._index] = self.acceptance_rates[: self._index]
            self.acceptance_rates = new_acceptance_rates
            self.max_size = new_max

        for i in range(n):
            if i == 200:
                self.burn(199)

            current = self.chain[self._index - 1]
            proposed = self.proposal_distribution.propose(current)

            # Use parent's acceptance ratio
            log_alpha = self.acceptance_ratio(current, proposed)

            if np.log(self.uniform_rng.uniform()) < log_alpha:
                self.chain[self._index] = proposed
                self.acceptance_count += 1
            else:
                self.chain[self._index] = current

            # Update covariance using current state
            if self._index % self.adaptation_interval == 0:
                self.update_covariance(self.chain[self._index])

            self.acceptance_rates[self._index] = self.acceptance_count / self._index
            self._index += 1


class AdaptiveMetropolisHastings(MetropolisHastings):
    def __init__(
        self,
        target: TargetDistribution,
        proposal: Proposal,
        initial_value: np.ndarray,
        adaptation_interval: int = 1,  # Changed to 1 based on Haario et al.
        target_acceptance: float = 0.234,  # Roberts & Rosenthal optimal
        adaptation_scale: float = 2.4,  # Haario et al. optimal
        min_samples_adapt: int = 800,  # Earlier adaptation from Roberts & Rosenthal
        max_samples_adapt: int = 1500,
    ):
        super().__init__(target, proposal, initial_value)
        self.adaptation_interval = adaptation_interval
        self.target_acceptance = target_acceptance

        # Optimal scaling from Haario et al.
        d = len(initial_value)
        self.adaptation_scale = (adaptation_scale**2) / d

        self.min_samples_adapt = min_samples_adapt
        self.max_samples_adapt = max_samples_adapt

        # Initialize empirical mean and covariance (Haario et al.)
        self.empirical_mean = initial_value.copy()
        self.empirical_cov = np.eye(d)
        self.n_samples = 1

    def update_empirical_estimates(self, new_sample: np.ndarray):
        """Update running covariance estimate using R&R 2009 method"""
        n = self._index - self.min_samples_adapt
        if n <= 0:
            return

        # Update mean
        old_mean = self.empirical_mean.copy()
        self.empirical_mean = (n * old_mean + new_sample) / (n + 1)

        # Update covariance using R&R formula
        self.empirical_cov = (n / (n + 1)) * self.empirical_cov + (
            n / ((n + 1) ** 2)
        ) * np.outer(old_mean - new_sample, old_mean - new_sample)

    def get_adaptation_weight(self) -> float:
        """Roberts & Rosenthal 2009 adaptation rate"""
        if self._index <= self.min_samples_adapt:
            return 1.0
        elif self._index >= self.max_samples_adapt:
            return 0.0

        t = self._index - self.min_samples_adapt
        # This gives rate decaying like 1/t
        return min(1.0, self.adaptation_scale / t)

    def update_proposal(self):
        """Update proposal covariance using R&R scheme"""
        if self._index < self.min_samples_adapt:
            return
        if self._index == self.min_samples_adapt:
            print(self.proposal_distribution.beta)
            return

        if self._index > self.max_samples_adapt:
            return
        if self._index == self.max_samples_adapt:
            print(self.proposal_distribution.beta)
            return

        scaled_cov = self.empirical_cov
        scaled_cov += 1e-6 * np.eye(self.empirical_cov.shape[0])

        self.proposal_distribution.beta = scaled_cov

    def __call__(self, n: int):
        """Run adaptive MCMC with online updates"""
        if self._index + n > self.max_size:
            new_max = max(self.max_size * 2, self._index + n)
            new_chain = np.empty((new_max, self.chain.shape[1]))
            new_chain[: self._index] = self.chain[: self._index]
            self.chain = new_chain
            self.max_size = new_max
            new_acceptance_rates = np.empty(new_max)
            new_acceptance_rates[: self._index] = self.acceptance_rates[: self._index]
            self.acceptance_rates = new_acceptance_rates
            self.max_size = new_max

        for i in range(n):
            if (self._index == self.min_samples_adapt // 2) and not self.burnt:
                print("Burning")
                self.burn(self.min_samples_adapt // 2 - 1)
            current = self.chain[self._index - 1]
            proposed = self.proposal_distribution.propose(current)

            log_alpha = self.acceptance_ratio(current, proposed)

            if np.log(self.uniform_rng.uniform()) < log_alpha:
                accepted_state = proposed
                self.chain[self._index] = proposed
                self.acceptance_count += 1
                # Update empirical estimates only on acceptance
            else:
                accepted_state = current
                self.chain[self._index] = current

            if (
                self._index % self.adaptation_interval == 0
                and self._index > self.min_samples_adapt
            ):
                self.update_empirical_estimates(accepted_state)
                self.update_proposal()

            self.acceptance_rates[self._index] = self.acceptance_count / self._index
            self._index += 1

================
File: BayesianInference/PRNG.py
================
from numpy.random import SeedSequence, Generator, Philox
from scipy.stats import rv_continuous
from typing import Optional

# Set Global Seed Here
SEED = 1000


# Parent RNG class
class RNG:
    def __init__(self, seed: Optional[int], distribution: rv_continuous):
        self.ss = SeedSequence(seed)
        self.rg = Generator(Philox(self.ss))  # Use Philox for parallel applications
        self.rng_distribution = distribution

    def __call__(self, loc, scale, size: Optional[int] = None, *args, **kwargs):
        return self.rng_distribution.rvs(
            loc, scale, size=size, random_state=self.rg, *args, **kwargs
        )

================
File: BayesianInference/setup.py
================
from setuptools import find_packages, setup

setup(
    name="BayesianInference",
    version="0.1",
    packages=find_packages(),
)

================
File: Experiments/Gaussian_Distribution_Inverse.py
================
import os
import sys
from typing import Tuple

import matplotlib.pyplot as plt
import numpy as np
import scipy.stats as sp
from scipy.fft import fft, fftfreq
from scipy.integrate import dblquad

sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

from BayesianInference.Diagnostics import \
    MCMCDiagnostics  # Using our new diagnostics file!
from BayesianInference.Distributions import Proposal, TargetDistribution
from BayesianInference.MetropolisHastings import (
    AdaptiveMetropolisHastings, AdaptiveMetropolisHastingsVOne,
    MetropolisHastings)


class MultivariateGaussianInverseTarget(TargetDistribution):
    def __init__(self, data, mean: np.ndarray, noise_sigma, prior_mean, prior_cov):
        """

        Mean is used for data generation
        """
        self.noise_sigma = noise_sigma

        self.prior = sp.multivariate_normal(prior_mean, prior_cov)
        self.likelihood = sp.multivariate_normal
        super().__init__(self.prior, self.likelihood, data, noise_sigma)

    def log_likelihood(self, x: np.ndarray) -> np.float64:
        """Log likelihood of data given parameters theta"""
        return super().log_likelihood(x)

    def log_prior(self, x: np.ndarray) -> np.float64:
        """Log prior on parameters"""
        t = super().log_prior(x)
        return t


class BananaTarget(TargetDistribution):
    """Banana-shaped distribution from Haario et al."""

    def __init__(self, b=0.1):
        self.b = b
        self.mean = np.array([0, -100 * b])
        self.cov = np.array([[1, 2 * b], [2 * b, 1 + 4 * b**2]])

    def log_likelihood(self, x: np.ndarray) -> np.float64:
        return self.log_prior(x)

    def log_prior(self, x: np.ndarray) -> np.float64:
        x1, x2 = x[0], x[1] + self.b * x[0] ** 2 - 100 * self.b
        return np.float64(-0.5 * (x1**2 + x2**2))


def run_inverse_example(n_samples: int = 50000):
    # Generate synthetic data from true parameter values
    mean = np.array([10.0, -0.2])
    sigma = np.array([[5.00, 2.5], [2.5, 5.00]])
    # noise = sp.norm.rvs(0, noise_sigma, size=2)
    # data = true_params + noise

    data = np.random.multivariate_normal(mean, sigma, 100)
    empirical_mean = np.mean(data, axis=0)
    empirical_cov = np.cov(data.T)

    # Setup target with generated data
    target = MultivariateGaussianInverseTarget(data, mean, sigma, mean, sigma // 2)

    # Setup sampler
    initial_state = np.zeros(2)
    proposal = Proposal(sp.multivariate_normal, scale=np.eye(2))

    sampler = AdaptiveMetropolisHastingsVOne(
        target=target,
        proposal=proposal,
        initial_value=initial_state,
        min_samples_adapt=500,
        max_samples_adapt=1000,
    )

    sampleTwo = AdaptiveMetropolisHastings(
        target=target,
        proposal=proposal,
        initial_value=initial_state,
        min_samples_adapt=500,
        max_samples_adapt=1000,
    )

    sampleThree = MetropolisHastings(
        target_distribution=target,
        proposal_distribution=Proposal(sp.multivariate_normal, 0.002 * sigma),
        initialstate=initial_state,
    )

    # Use our diagnostics class!

    # sampler(n_samples)
    # diagnostics = MCMCDiagnostics(sampler)
    #   diagnostics.plot_diagnostics()
    #   diagnostics.plot_target_distribution()
    #   diagnostics.print_summary()

    sampleTwo(n_samples)
    diagnostics = MCMCDiagnostics(sampleTwo)
    diagnostics.plot_diagnostics()
    # diagnostics.plot_target_distribution()
    diagnostics.print_summary()

    # sampleThree(n_samples)
    # diagnostics = MCMCDiagnostics(sampleThree)
    # diagnostics.plot_diagnostics()
    ## diagnostics.plot_target_distribution()
    # diagnostics.print_summary()
    print(f"True Mean: {mean}, True Cov: {sigma}")


if __name__ == "__main__":
    run_inverse_example()

================
File: Experiments/Gaussian_Mixtures.py
================
import sys
import os
import numpy as np
import scipy.stats as sp
from typing import Tuple

sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))
from BayesianInference.Distributions import Proposal, TargetDistribution
from BayesianInference.MetropolisHastings import MetropolisHastings, AdaptiveMetropolisHastings
from BayesianInference.Diagnostics import MCMCDiagnostics

class DoubleWellTarget(TargetDistribution):
    """Bimodal target using double-well potential"""
    def __init__(self, sigma: float = 0.3):
        self.sigma = sigma
        
        # Setup prior and likelihood for double-well structure
        prior = sp.multivariate_normal(mean=np.zeros(2), cov=np.eye(2))
        likelihood = sp.multivariate_normal
        data = None  # No actual data needed for this test case
        
        super().__init__(prior, likelihood, data, sigma)
        
        # Store analytical values
        self._analytical_mean = np.array([0.0, 0.0])
        self._analytical_var = np.array([[1.0, 0.0], [0.0, 1.0]]) * self.sigma**2

    def log_likelihood(self, x: np.ndarray) -> np.float64:
        """Double-well potential in first dimension"""
        return np.float64(-0.25/(self.sigma**2) * (x[0]**2 - 1)**2)

    def log_prior(self, x: np.ndarray) -> np.float64:
        """Double-well potential in second dimension"""
        return np.float64(-0.25/(self.sigma**2) * (x[1]**2 - 1)**2)

    def analytical_mean(self) -> np.ndarray:
        """True mean by symmetry"""
        return self._analytical_mean

    def analytical_covariance(self) -> np.ndarray:
        """True covariance by symmetry and numerical integration"""
        return self._analytical_var

def setup_samplers(
    target: TargetDistribution,
    initial_state: np.ndarray,
    mh_scale: float = 0.3
) -> Tuple[MetropolisHastings, AdaptiveMetropolisHastings]:
    """Setup both MH and Adaptive MH samplers with proper parameters"""
    
    # Standard MH with fixed proposal
    mh_proposal = Proposal(sp.multivariate_normal, scale=mh_scale*np.eye(2))
    mh_sampler = MetropolisHastings(target, mh_proposal, initial_state)
    
    # Adaptive MH with proper parameters
    amh_proposal = Proposal(sp.multivariate_normal, scale=np.eye(2))
    amh_sampler = AdaptiveMetropolisHastings(
        target=target,
        proposal=amh_proposal,
        initial_value=initial_state,
        adaptation_interval=5,
        target_acceptance=0.234,
        adaptation_scale=2.4,
        min_samples_adapt=2000,
        max_samples_adapt=3500
    )
    
    return mh_sampler, amh_sampler

def run_comparison(n_samples: int = 50000) -> Tuple[MetropolisHastings, AdaptiveMetropolisHastings, TargetDistribution]:
    """Run and compare MCMC samplers on bimodal target"""
    
    # Validate inputs
    if n_samples < 5000:
        raise ValueError("n_samples should be at least 5000 for reliable results")
    
    # Setup target and samplers
    target = DoubleWellTarget(sigma=0.3)
    initial_state = np.zeros(2)
    mh_sampler, amh_sampler = setup_samplers(target, initial_state)
    
    # Run samplers
    print("Running Standard MH...")
    mh_sampler(n_samples)
    print("Running Adaptive MH...")
    amh_sampler(n_samples)
    
    # Analyze results
    print("\nAnalytical Values:")
    print(f"Mean: {target.analytical_mean()}")
    print(f"Covariance:\n{target.analytical_covariance()}")
    
    print("\nStandard MH Diagnostics:")
    mh_diag = MCMCDiagnostics(mh_sampler, target.analytical_mean())
    mh_diag.plot_diagnostics()
    #mh_diag.plot_target_distribution()
    mh_diag.print_summary()
    
    print("\nAdaptive MH Diagnostics:")
    amh_diag = MCMCDiagnostics(amh_sampler, target.analytical_mean())
    amh_diag.plot_diagnostics()
    #amh_diag.plot_target_distribution()
    amh_diag.print_summary()
    
    return mh_sampler, amh_sampler, target

if __name__ == "__main__":
    mh, amh, target = run_comparison()

================
File: HeatDiffusion/HeatDiffusion1D.py
================
import os
import sys
from dataclasses import dataclass
from typing import Callable, Optional, Tuple

sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

import matplotlib.pyplot as plt
import numpy as np
import scipy.stats as sp
from BayesianInference.Distributions import Proposal, TargetDistribution
from BayesianInference.MetropolisHastings import MetropolisHastings
from BayesianInference.PRNG import RNG
from scipy.sparse import csc_matrix, diags
from scipy.sparse.linalg import spsolve


@dataclass
class DirichletHeatConfig:
    """Configuration for heat equation with Dirichlet BCs"""

    L: float = 1.0  # Domain length
    T: float = 1.0  # Final time
    nx: int = 100  # Number of spatial points
    nt: int = 100  # Number of time points
    left_bc: Callable = lambda t: 0  # Left boundary condition
    right_bc: Callable = lambda t: 0  # Right boundary condition


class DirichletHeatSolver:
    """
    Solver for heat equation with Dirichlet boundary conditions using
    Crank-Nicolson scheme
    """

    def __init__(self, config: DirichletHeatConfig):
        self.config = config

        # Setup grid
        self.dx = config.L / (config.nx - 1)
        self.dt = config.T / config.nt
        self.x = np.linspace(0, config.L, config.nx)
        self.t = np.linspace(0, config.T, config.nt)

        # Stability parameter
        self.r = self.dt / (self.dx**2)
        if self.r > 0.5:
            print(f"Warning: Grid Fourier number {self.r} > 0.5")
            print("Solution may be unstable")

        # Setup Crank-Nicolson matrices
        self._setup_matrices()

    def _setup_matrices(self):
        """Setup matrices for Crank-Nicolson scheme"""
        nx = self.config.nx
        r = self.r

        # Interior points only (nx-2 points)
        main_diag = (1 + r) * np.ones(nx - 2)
        off_diag = -0.5 * r * np.ones(nx - 3)

        # LHS matrix (implicit part)
        self.A = diags(
            [off_diag, main_diag, off_diag],
            [-1, 0, 1],
            shape=(nx - 2, nx - 2),
            format="csc",
        )

        # RHS matrix (explicit part)
        self.B = diags(
            [-off_diag, (2 - main_diag), -off_diag],
            [-1, 0, 1],
            shape=(nx - 2, nx - 2),
            format="csc",
        )

    def solve(self, initial_condition: np.ndarray) -> np.ndarray:
        """
        Solve the heat equation

        Args:
            initial_condition: Initial temperature distribution

        Returns:
            u: Solution array of shape (nt, nx)
        """
        nx, nt = self.config.nx, self.config.nt
        u = np.zeros((nt, nx))
        r = self.r

        # Set initial condition
        u[0] = initial_condition

        # Time stepping
        for k in range(nt - 1):
            # Current time
            t_now = self.t[k]
            t_next = self.t[k + 1]

            # Get interior points
            interior = u[k, 1:-1]

            # RHS vector
            b = self.B @ interior

            # Add boundary contributions
            # Left boundary
            b[0] += 0.5 * r * (self.config.left_bc(t_now) + self.config.left_bc(t_next))

            # Right boundary
            b[-1] += (
                0.5 * r * (self.config.right_bc(t_now) + self.config.right_bc(t_next))
            )

            # Solve for interior points
            u[k + 1, 1:-1] = spsolve(self.A, b)

            # Update boundary values
            u[k + 1, 0] = self.config.left_bc(t_next)
            u[k + 1, -1] = self.config.right_bc(t_next)

        return u

    def analytical_solution(x: np.ndarray, t: np.ndarray) -> np.ndarray:
        """Compute analytical solution u(x,t) = sin(pix)exp(-pi^2t)"""
        X, T = np.meshgrid(x, t)
        return np.sin(np.pi * X) * np.exp(-(np.pi**2) * T)

    def compute_error(
        self, numerical: np.ndarray, analytical: np.ndarray
    ) -> np.ndarray:
        """Compute absolute error between numerical and analytical solutions"""
        return np.abs(numerical - analytical)


class DirichletHeatInverse(TargetDistribution):
    """
    Inverse problem for heat equation with Dirichlet BCs
    """

    def __init__(
        self,
        solver: DirichletHeatSolver,
        observations: np.ndarray,
        observation_times: np.ndarray,
        observation_locs: np.ndarray,
        sigma: float,
        prior_mean: Optional[np.ndarray] = None,
        prior_std: Optional[float] = None,
    ):
        self.sigma = sigma
        super().__init__(
            prior=sp.norm, likelihood=sp.norm, data=observations, sigma=sigma
        )
        self.solver = solver
        self.obs_times = observation_times
        self.obs_locs = observation_locs

        # Find indices for observation times and locations
        self.time_indices = [np.abs(solver.t - t).argmin() for t in observation_times]
        self.space_indices = [np.abs(solver.x - x).argmin() for x in observation_locs]

        # Set prior parameters
        nx = solver.config.nx
        self.prior_mean = prior_mean if prior_mean is not None else np.zeros(nx)
        self.prior_std = prior_std if prior_std is not None else np.ones(nx)

    def log_likelihood(self, x: np.ndarray) -> np.float64:
        """Compute log likelihood"""
        try:
            # Ensure boundary conditions match
            if not np.isclose(x[0], self.solver.config.left_bc(0)) or not np.isclose(
                x[-1], self.solver.config.right_bc(0)
            ):
                return np.float64(-np.inf)

            # Solve forward problem
            solution = self.solver.solve(x)

            # Extract solution at observation points
            predicted = solution[np.ix_(self.time_indices, self.space_indices)]

            # Compute log likelihood
            residuals = predicted - self.data
            return super().log_likelihood.logpdf(residuals, scale=self.sigma)
        except:
            return np.float64(-np.inf)

    def log_prior(self, x: np.ndarray) -> np.float64:
        """Compute log prior"""
        # Check boundary conditions
        if not np.isclose(x[0], self.solver.config.left_bc(0)) or not np.isclose(
            x[-1], self.solver.config.right_bc(0)
        ):
            return np.float64(-np.inf)

        # Compute prior only for interior points
        return np.sum(super().log_prior(x))


def test_dirichlet_heat():
    """Test heat equation solver with Dirichlet BCs"""

    # 1. Setup problem configuration
    config = DirichletHeatConfig(
        L=1.0,
        T=1.0,
        nx=50,
        nt=5000,
        left_bc=lambda t: 0,  # Zero boundary conditions
        right_bc=lambda t: 0,
    )

    solver = DirichletHeatSolver(config)

    # 2. Generate synthetic data
    # True initial condition (satisfy boundary conditions)
    x = solver.x
    true_ic = np.sin(np.pi * x)  # Satisfies zero BCs

    # Solve forward problem
    true_solution = solver.solve(true_ic)

    # Create observations
    noise_std = 0.05
    obs_times = np.linspace(0, config.T, 10)
    obs_locs = np.linspace(0, config.L, 20)[1:-1]  # Exclude boundaries

    # Generate noisy observations
    obs_t_idx = [np.abs(solver.t - t).argmin() for t in obs_times]
    obs_x_idx = [np.abs(solver.x - x).argmin() for x in obs_locs]

    observations = true_solution[
        np.ix_(obs_t_idx, obs_x_idx)
    ] + noise_std * np.random.randn(len(obs_times), len(obs_locs))

    # 3. Setup inverse problem
    target = DirichletHeatInverse(
        solver=solver,
        observations=observations,
        observation_times=obs_times,
        observation_locs=obs_locs,
        sigma=noise_std,
        prior_mean=np.zeros_like(x),
        prior_std=1.0,
    )

    # 4. Setup MCMC
    # Initial guess (satisfying BCs)
    initial_state = 0.5 * np.sin(2 * np.pi * x)

    # Proposal (only varies interior points)
    proposal_scale = 0.1 * np.eye(config.nx - 2)  # For interior points
    proposal = Proposal(sp.multivariate_normal, scale=proposal_scale)

    # Modified proposal to handle boundary conditions
    def propose_with_bcs(current):
        # Propose new interior points
        proposed_interior = proposal.propose(current[1:-1])
        # Keep boundary values fixed
        proposed = np.zeros_like(current)
        proposed[0] = config.left_bc(0)
        proposed[-1] = config.right_bc(0)
        proposed[1:-1] = proposed_interior
        return proposed

    proposes = Proposal(sp.norm, 5)

    # 5. Run MCMC
    mcmc = MetropolisHastings(target, proposes, initial_state)
    mcmc(5000)

    # 6. Plot results
    plt.figure(figsize=(15, 5))

    # Plot initial condition reconstruction
    plt.subplot(121)
    plt.plot(x, true_ic, "k-", label="True")
    plt.plot(x, mcmc.chain[-1000:].mean(axis=0), "r--", label="Posterior Mean")
    plt.fill_between(
        x,
        np.percentile(mcmc.chain[-1000:], 5, axis=0),
        np.percentile(mcmc.chain[-1000:], 95, axis=0),
        color="r",
        alpha=0.2,
        label="90% Credible Interval",
    )
    plt.legend()
    plt.title("Initial Condition Recovery")

    # Plot observations and predictions
    plt.subplot(122)
    plt.plot(x, true_solution.T, "k-", alpha=0.1)
    plt.scatter(
        obs_locs.repeat(len(obs_times)),
        observations.flatten(),
        c="r",
        alpha=0.2,
        s=10,
        label="Observations",
    )

    # Plot some predictions from posterior
    for ic in mcmc.chain[-1000::100]:
        pred = solver.solve(ic)
        plt.plot(x, pred.T, "b-", alpha=0.1)

    plt.legend()
    plt.title("Observations and Predictions")
    plt.tight_layout()
    plt.show()


if __name__ == "__main__":
    test_dirichlet_heat()

================
File: HeatDiffusion/HeatDiffusion1DTest.py
================
import matplotlib.pyplot as plt
import numpy as np
from HeatDiffusion1D import DirichletHeatConfig, DirichletHeatSolver


def test_heat_solver():
    """
    Test heat equation solver against analytical solution:
    u(x,t) = sin(πx)exp(-π²t)
    """
    # Setup configuration with better stability
    config = DirichletHeatConfig(
        L=1.0,
        T=1.0,
        nx=50,
        nt=5000,  # Increased time steps to reduce Fourier number
        left_bc=lambda t: 0,
        right_bc=lambda t: 0,
    )

    solver = DirichletHeatSolver(config)

    # Print grid parameters
    print(f"dx = {solver.dx:.6f}")
    print(f"dt = {solver.dt:.6f}")
    print(f"Fourier number = {solver.r:.6f}")

    # Initial condition
    x = solver.x
    initial_condition = np.sin(np.pi * x)

    # Solve numerically
    numerical_solution = solver.solve(initial_condition)

    # Compute analytical solution
    def analytical_solution(x, t):
        return np.sin(np.pi * x) * np.exp(-(np.pi**2) * t)

    X, T = np.meshgrid(x, solver.t)
    analytical = analytical_solution(X, T)

    # Compute error
    error = np.abs(numerical_solution - analytical)
    max_error = np.max(error)
    l2_error = np.sqrt(np.mean(error**2))

    print(f"Maximum error: {max_error:.2e}")
    print(f"L2 error: {l2_error:.2e}")

    # Create separate figures for better stability
    # Figure 1: Solutions at different times
    plt.figure(1)
    times = [0, 0.2, 0.5, 1.0]
    for t in times:
        tidx = np.abs(solver.t - t).argmin()
        plt.plot(x, numerical_solution[tidx], "--", label=f"t={t:.1f} (num)")
        plt.plot(x, analytical_solution(x, t), "k:", label=f"t={t:.1f} (exact)")
    plt.legend()
    plt.title("Solution at Different Times")
    plt.xlabel("x")
    plt.ylabel("u(x,t)")
    plt.grid(True)
    plt.savefig("Assets/heat_solutions.png")
    plt.close()

    # Figure 2: Error distribution
    plt.figure(2)
    plt.pcolormesh(X, T, error, shading="auto")
    plt.colorbar(label="Absolute Error")
    plt.xlabel("x")
    plt.ylabel("t")
    plt.title("Error Distribution")
    plt.savefig("Assets/heat_error_dist.png")
    plt.close()

    # Figure 3: Error evolution
    plt.figure(3)
    plt.semilogy(solver.t, np.max(error, axis=1), label="Max Error")
    plt.semilogy(solver.t, np.sqrt(np.mean(error**2, axis=1)), label="L2 Error")
    plt.legend()
    plt.xlabel("t")
    plt.ylabel("Error")
    plt.title("Error Evolution")
    plt.grid(True)
    plt.savefig("Assets/heat_error_evolution.png")
    plt.close()

    return numerical_solution, analytical, error


if __name__ == "__main__":
    numerical, analytical, error = test_heat_solver()
    print("\nTest completed.")

================
File: MetropolisHastings/BIP_Oscillator.py
================
import matplotlib.pyplot as plt
import numpy as np
import scipy as sp
from Implementations.BayesianInference import SEED
from Implementations.BayesianInference import TargetDistribution, Proposal
from Implementations.BayesianInference.MetropolisHastings import MetropolisHastings

# The static SEED variable for reproducibility
Seed = 1

# True parameter values for damped oscillator
true_params = np.array([0.2, 2.0])  # [damping, frequency]


# Generate synthetic data
def oscillator(t, params):
    """Damped oscillator solution"""
    damping, freq = params
    return np.exp(-damping * t) * np.cos(freq * t)


# Generate data points
t_obs = np.linspace(0, 10, 100)
true_signal = oscillator(t_obs, true_params)
noise_level = 0.1
noise_rng = np.random.Generator(np.random.Philox(np.random.SeedSequence(SEED)))

noisy_data = true_signal + noise_rng.normal(0, noise_level, size=len(t_obs))


class OscillatorTarget(TargetDistribution):
    def __init__(self, data, t, noise_sigma):
        # Use parent's structure as is
        prior = sp.stats.uniform(loc=[0, 0], scale=[1, 5])
        likelihood = sp.stats.norm
        super().__init__(prior, likelihood, data, noise_sigma)
        self.t = t

    def forward_model(self, params: np.ndarray) -> np.ndarray:
        """Transform parameters to predictions"""
        return oscillator(self.t, params)

    # Parent log_likelihood and log_prior stay the same!
    # Just use forward_model when needed:
    def log_likelihood(self, x: np.ndarray) -> np.float64:
        predicted = self.forward_model(x)
        return super().log_likelihood(predicted)

    def log_prior(self, x):
        return np.sum(super().log_prior(x))  # Sum the log probabilities


# Set up MCMC
target = OscillatorTarget(noisy_data, t_obs, noise_level)
proposal = Proposal(
    sp.stats.multivariate_normal, scale=np.array([[0.1, 0], [0, 0.1]])
)  # Small step size

# Initial guess
initial_state = np.array([0.1, 1.5])

# Run MCMC
mcmc = MetropolisHastings(target, proposal, initial_state)
mcmc(5000)

# Plot results
plt.figure(figsize=(12, 4))

# Plot data and fit
plt.subplot(121)
plt.plot(t_obs, noisy_data, "k.", label="Data")
plt.plot(t_obs, true_signal, "g-", label="True")
final_params = mcmc.chain[4000]  # Use a late sample
plt.plot(t_obs, oscillator(t_obs, final_params), "r--", label="Estimated")
plt.legend()
plt.title("Data and Fit")

# Plot parameter traces
plt.subplot(122)
plt.plot(mcmc.chain[:4000, 0], mcmc.chain[:4000, 1], "k.", alpha=0.1)
plt.plot(true_params[0], true_params[1], "r*", markersize=10)
plt.xlabel("Damping")
plt.ylabel("Frequency")
plt.title("Parameter Space")

plt.show()

================
File: ReactionDiffusion.py
================
import numpy as np
import scipy.stats as sp
from scipy.integrate import solve_bvp
import matplotlib.pyplot as plt
import os
import sys

sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

from BayesianInference.PRNG import RNG, SEED
from BayesianInference.Distributions import TargetDistribution, Proposal
from BayesianInference.MetropolisHastings import MetropolisHastings


class ReactionDiffusionTarget(TargetDistribution):
    """Target distribution for the reaction-diffusion inverse problem"""

    def __init__(self, data, x_points, noise_sigma, boundary_conditions):
        # Set up the prior for [r1, r2, D] parameters
        prior = sp.uniform(loc=[0, 0, 0], scale=[5, 5, 1])
        likelihood = sp.t  # Using t-distribution with 6 degrees of freedom
        super().__init__(prior, likelihood, data, noise_sigma)

        self.x = x_points
        self.bc_a, self.bc_b = boundary_conditions

    def log_likelihood(self, x) -> np.float64:
        """Override parent log_likelihood to handle solver failures"""
        predicted = self.forward_model(x)
        if np.any(np.isinf(predicted)):
            return np.float64(-np.inf)
        # Use t-distribution with 6 degrees of freedom
        return np.float64(
            np.sum(
                self.likelihood.logpdf(
                    self.data - predicted,  # residuals
                    df=4,  # degrees of freedom
                    loc=0,  # mean
                    scale=self.data_sigma,  # scale
                )
            )
        )

    def log_prior(self, x: np.ndarray) -> np.float64:
        """Log prior probability"""
        if np.any(x <= 0):  # Parameters must be positive
            return np.float64(-np.inf)
        return np.float64(np.sum(super().log_prior(x)))

    def forward_model(self, params):
        solution = self.solve_steady_state(params)
        if solution is None:
            return np.full_like(self.data, np.inf)
        return solution

    def solve_steady_state(self, params):
        """Solve the steady state reaction-diffusion equation"""
        D = params[2]  # Diffusion coefficient

        def ode_system(x, y):
            # y[0] is u, y[1] is u'
            return np.vstack((y[1], -self.reaction_term(y[0], params) / D))

        def boundary_conditions(ya, yb):
            return np.array([ya[0] - self.bc_a, yb[0] - self.bc_b])

        # Initial guess for solution
        y = np.zeros((2, len(self.x)))
        y[0] = np.linspace(self.bc_a, self.bc_b, len(self.x))

        try:
            sol = solve_bvp(ode_system, boundary_conditions, self.x, y)
            if not sol.success:
                return None
            return sol.sol(self.x)[0]
        except:
            return None

    def reaction_term(self, u, params):
        """Cubic reaction term R(u) = r1*u - r2*u^3"""
        r1, r2, _ = params
        return r1 * u - r2 * u**3


def generate_synthetic_data(x_points, true_params, noise_sigma, boundary_conditions):
    """Generate synthetic data for testing"""
    model = ReactionDiffusionTarget(None, x_points, noise_sigma, boundary_conditions)
    true_solution = model.solve_steady_state(true_params)

    if true_solution is None:
        raise ValueError("Failed to solve ODE for true parameters")

    # Use the configured PRNG from your library
    noise_rng = RNG(SEED, sp.norm)
    noisy_data = true_solution + noise_rng(0, noise_sigma, len(true_solution))
    return noisy_data, true_solution


if __name__ == "__main__":
    # Problem setup
    x_points = np.linspace(0, 1, 100)
    noise_sigma = 0.2
    true_params = np.array([2.0, 1.0, 0.1])  # [r1, r2, D]
    boundary_conditions = (0.2, 0.2)  # Non-zero but moderate boundary conditions

    # Generate synthetic data
    noisy_data, true_solution = generate_synthetic_data(
        x_points, true_params, noise_sigma, boundary_conditions
    )
    print("True solution shape:", true_solution.shape)
    print("True solution range:", np.min(true_solution), np.max(true_solution))

    # Set up MCMC with same boundary conditions
    target = ReactionDiffusionTarget(
        noisy_data, x_points, noise_sigma, boundary_conditions
    )
    scale = np.diag([0.6, 0.6, 0.1])
    proposal = Proposal(sp.multivariate_normal, scale=scale)
    initial_state = np.array([1.5, 0.8, 0.08])
    mcmc = MetropolisHastings(target, proposal, initial_state)

    # Run MCMC
    n_iterations = 50000
    mcmc(n_iterations)

    # Debug chain shape
    print("Chain shape:", mcmc.chain.shape)
    print("Active chain shape:", mcmc.chain[: mcmc._index].shape)

    # Plot results
    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 5))

    # Parameter traces with fixed x-axis - only plot actual chain values
    active_chain = mcmc.chain[: mcmc._index]  # Only use actual samples
    for i, (param, label) in enumerate(zip(active_chain.T, ["r₁", "r₂", "D"])):
        ax1.plot(np.arange(len(param)), param, label=label)
    ax1.set_xlabel("Iteration")
    ax1.set_ylabel("Parameter Value")
    ax1.set_title("Parameter Traces")
    ax1.legend()

    # Data and fit comparison
    ax2.plot(x_points, noisy_data, "k.", alpha=0.5, label="Data")
    ax2.plot(x_points, true_solution, "g-", label="True")

    final_params = active_chain[-1]  # Use last actual sample
    print("\nFinal parameters:", final_params)
    final_solution = target.solve_steady_state(final_params)
    print("Final solution exists:", final_solution is not None)
    if final_solution is not None:
        print("Final solution range:", np.min(final_solution), np.max(final_solution))
        ax2.plot(x_points, final_solution, "r--", label="MCMC Fit")

    ax2.set_xlabel("x")
    ax2.set_ylabel("u(x)")
    ax2.set_title("Data and Model Fit")
    ax2.legend()

    plt.tight_layout()
    plt.show()

    # Print results using chain values
    print("\nResults:")
    print("True parameters:", true_params)
    print("Estimated parameters (mean of last 100 samples):")
    print(np.mean(active_chain[-100:], axis=0))
    print("\nParameter standard deviations:")
    print(np.std(active_chain[-100:], axis=0))

    plt.hist(
        mcmc.chain[: mcmc._index],
        bins=30,
        density=True,
        histtype="step",
        label=["r1", "r2", "D"],
    )
    plt.title("Parameter Distribution after Sampling")
    plt.xlabel("Parameter Value")
    plt.legend()
    plt.show()

================
File: README.md
================
Written by Finbar Cowan from 2 semester research project on Bayesian Inverse Problems with a focus on MCMC methods for sampling approximately/directly/exactly from the Posterior distribution.

Work in Progress.

For most up to date and interesting part of the project, please run ReactionDiffusion.py

================
File: TestStubs/MetropolisHastingsTestStub.py
================
import sys
import os

import numpy as np
import scipy.stats as stats
import unittest
sys.path.append(os.path.abspath(os.path.join(os.path.dirname(__file__), '../MetropolisHastings')))
from MetropolisHastings import RNG, Proposal, TargetDistribution, MetropolisHastings  # Import your classes here


class TestRNG(unittest.TestCase):
    def setUp(self):
        self.seed = 42
        self.n = 5
        self.distribution = stats.norm
        self.rng = RNG(self.seed, self.n, self.distribution)

    def test_rng_generation(self):
        loc = 0
        scale = 1
        samples = self.rng(loc=loc, scale=scale)
        self.assertEqual(samples.shape, (self.n,))
        self.assertTrue(np.all(np.isfinite(samples)))


class TestProposal(unittest.TestCase):
    def setUp(self):
        self.seed = 42
        self.parameter = np.array([0.0])
        self.proposal_distribution = stats.norm
        self.proposal = Proposal(self.proposal_distribution, self.parameter)

    def test_propose(self):
        current = np.array([1.0])
        proposal_samples = self.proposal.propose(current)
        self.assertEqual(proposal_samples.shape, (self.proposal.proposal_dimension,))
        self.assertTrue(np.all(np.isfinite(proposal_samples)))


class TestTargetDistribution(unittest.TestCase):
    def setUp(self):
        self.prior = stats.norm(loc=0, scale=1)
        self.likelihood = stats.norm(loc=1, scale=1)
        self.data = 1.5
        self.sigma = 0.5
        self.target_dist = TargetDistribution(self.prior, self.likelihood, self.data, self.sigma)

    def test_log_likelihood(self):
        x = np.array([1.0])
        log_likelihood = self.target_dist.log_likelihood(x)
        self.assertTrue(np.isfinite(log_likelihood))

    def test_log_prob(self):
        x = np.array([1.0])
        log_prob = self.target_dist.log_prob(x)
        self.assertTrue(np.isfinite(log_prob))


class TestMetropolisHastings(unittest.TestCase):
    def setUp(self):
        self.target_dist = TargetDistribution(
            prior=stats.norm(loc=0, scale=1),
            likelihood=stats.norm(loc=1, scale=1),
            data=1.5,
            sigma=0.5
        )
        self.proposal_dist = stats.norm(loc=0, scale=1)
        self.initial_state = np.array([0.0])
        self.mh = MetropolisHastings(self.target_dist, self.proposal_dist, self.initial_state)

    def test_initialization(self):
        self.assertTrue(np.array_equal(self.mh.initial_state, self.initial_state))
        self.assertEqual(self.mh.chain.shape, (1, 1))  # Initial state shape should be (1, dimension)

    def test_prior(self):
        # Implement a simple test for the prior method
        # Note: This method currently has no implementation; this is just a placeholder
        pass


if __name__ == "__main__":
    unittest.main()
