This file is a merged representation of the entire codebase, combining all repository files into a single document.
Generated by Repomix on: 2025-01-31T06:19:29.894Z

================================================================
File Summary
================================================================

Purpose:
--------
This file contains a packed representation of the entire repository's contents.
It is designed to be easily consumable by AI systems for analysis, code review,
or other automated processes.

File Format:
------------
The content is organized as follows:
1. This summary section
2. Repository information
3. Directory structure
4. Multiple file entries, each consisting of:
  a. A separator line (================)
  b. The file path (File: path/to/file)
  c. Another separator line
  d. The full contents of the file
  e. A blank line

Usage Guidelines:
-----------------
- This file should be treated as read-only. Any changes should be made to the
  original repository files, not this packed version.
- When processing this file, use the file path to distinguish
  between different files in the repository.
- Be aware that this file may contain sensitive information. Handle it with
  the same level of security as you would the original repository.

Notes:
------
- Some files may have been excluded based on .gitignore rules and Repomix's
  configuration.
- Binary files are not included in this packed representation. Please refer to
  the Repository Structure section for a complete list of file paths, including
  binary files.

Additional Info:
----------------

================================================================
Directory Structure
================================================================
Diagnostics.py
Distributions.py
GradientUtils.py
MALA.py
MetropolisHastings.py
PRNG.py

================================================================
Files
================================================================

================
File: Diagnostics.py
================
from typing import Dict, Optional, Tuple

import matplotlib.pyplot as plt
import numpy as np
import scipy as sp
from scipy.stats import gaussian_kde

from LibMCMC.MetropolisHastings import MetropolisHastings


class MCMCDiagnostics:
    """Comprehensive MCMC diagnostics and visualization"""

    def __init__(self, sampler: MetropolisHastings, true_value):
        self.sampler = sampler
        self.chain = sampler.chain[: sampler._index]
        self.n_samples = sampler._index
        self.n_params = self.chain.shape[1]
        self.diagnostics = self._calculate_diagnostics()

    # Core statistical computations
    def _compute_ess(self, x: np.ndarray) -> float:
        """Compute effective sample size for a parameter chain"""
        n = len(x)
        if n <= 1:
            return 0
        acf = np.correlate(x - np.mean(x), x - np.mean(x), mode="full")[n - 1 :]
        acf = acf / acf[0]
        cutoff = np.where((acf < 0) | (acf < 0.05))[0]
        cutoff = cutoff[0] if len(cutoff) > 0 else len(acf)
        ess = n / (1 + 2 * np.sum(acf[1:cutoff]))
        return max(1, ess)

    def _compute_act(self, x: np.ndarray) -> float:
        """Compute integrated autocorrelation time"""
        ess = self._compute_ess(x)
        return len(x) / ess if ess > 0 else np.inf

    def _compute_autocorr(
        self, param_idx: int, max_lag: Optional[int] = None
    ) -> np.ndarray:
        """Compute autocorrelation for a parameter"""
        if max_lag is None:
            max_lag = min(100, self.n_samples // 5)

        acf = []
        x = self.chain[:, param_idx]
        for k in range(max_lag):
            if len(x[k:]) > 0 and len(x[:-k]) > 0:
                correlation = np.corrcoef(x[:-k], x[k:])[0, 1]
                acf.append(correlation)
        return np.array(acf)

    def _calculate_diagnostics(self) -> Dict:
        """Calculate all MCMC diagnostics"""
        # Basic statistics
        mean_estimate = np.mean(self.chain, axis=0)
        cov_estimate = np.cov(self.chain.T)

        # Advanced statistics
        ess = np.array(
            [self._compute_ess(self.chain[:, i]) for i in range(self.n_params)]
        )
        act = np.array(
            [self._compute_act(self.chain[:, i]) for i in range(self.n_params)]
        )

        # R-hat calculation
        n_split = self.n_samples // 2
        chains = [self.chain[:n_split], self.chain[n_split:]]
        within_var = np.mean([np.var(c, axis=0) for c in chains], axis=0)
        chain_means = np.array([np.mean(c, axis=0) for c in chains])
        between_var = np.var(chain_means, axis=0) * n_split
        r_hat = np.sqrt((within_var + between_var / n_split) / within_var)

        return {
            "mean_estimate": mean_estimate,
            "covariance_estimate": cov_estimate,
            "ess": ess,
            "act": act,
            "r_hat": r_hat,
            "acceptance_rate": self.sampler.acceptance_count / self.n_samples,
            "n_samples": self.n_samples,
        }

    # Plotting methods
    def _plot_trajectory(self, ax: plt.Axes):
        """Plot chain trajectory"""
        ax.plot(self.chain[:, 0], self.chain[:, 1], "k.", alpha=0.1, markersize=1)
        ax.set_xlabel("X1")
        ax.set_ylabel("X2")
        ax.set_title("Chain Trajectory")

    def _plot_traces(self, ax: plt.Axes):
        """Plot trace plots with running means"""
        for i in range(self.n_params):
            ax.plot(self.chain[:, i], label=f"X{i+1}", alpha=0.5)
            running_mean = np.cumsum(self.chain[:, i]) / np.arange(
                1, self.n_samples + 1
            )
            ax.plot(running_mean, "--", label=f"Mean X{i+1}")
        ax.set_xlabel("Iteration")
        ax.set_title("Trace Plots")
        ax.legend()

    def _plot_autocorr(self, ax: plt.Axes):
        """Plot autocorrelation"""
        max_lag = min(100, self.n_samples // 5)
        for i in range(self.n_params):
            acf = self._compute_autocorr(i, max_lag)
            ax.plot(acf, label=f"X{i+1}")
        ax.set_title("Autocorrelation")
        ax.legend()

    def _plot_acceptance_rate(self, ax: plt.Axes):
        """Plot running acceptance rate"""
        #        window = min(500, self.n_samples // 20)
        #        acc_rates = [np.mean(np.diff(self.chain[i:i+window, 0]) != 0)
        #                    for i in range(0, self.n_samples-window, window)]
        #        ax.plot(range(0, self.n_samples-window, window), acc_rates)
        ax.plot(
            range(0, self.n_samples), self.sampler.acceptance_rates[: self.n_samples]
        )
        ax.axhline(y=0.234, color="r", linestyle="--", label="Optimal")
        ax.set_xlabel("Iteration")
        ax.set_ylabel("Acceptance Rate")
        ax.set_title("Running Acceptance Rate")
        ax.legend()

    def _plot_ess(self, ax: plt.Axes):
        """Plot effective sample size"""
        ax.bar([f"X{i+1}" for i in range(self.n_params)], self.diagnostics["ess"])
        ax.set_ylabel("ESS")
        ax.set_title("Effective Sample Size")

    def _plot_marginals(self, ax: plt.Axes):
        """Plot marginal distributions"""
        for i in range(self.n_params):
            ax.hist(self.chain[:, i], bins=50, density=True, alpha=0.5, label=f"X{i+1}")
        ax.set_title("Marginal Distributions")
        ax.legend()

    # Main public methods
    def plot_diagnostics(self, show: bool = True) -> Tuple[plt.Figure, np.ndarray]:
        """Create comprehensive diagnostic plots"""
        fig, axes = plt.subplots(2, 3, figsize=(15, 10))

        self._plot_trajectory(axes[0, 0])
        self._plot_traces(axes[0, 1])
        self._plot_autocorr(axes[0, 2])
        self._plot_acceptance_rate(axes[1, 0])
        self._plot_ess(axes[1, 1])
        self._plot_marginals(axes[1, 2])
        plt.tight_layout()
        fig.savefig(f"./Plots/MCMC_plot_{type(self.sampler).__name__}")
        if show:
            plt.show()
        return fig, axes

    def print_summary(self):
        """Print comprehensive MCMC diagnostics summary"""
        print("\nMCMC Summary Statistics:")
        print("=" * 50)

        print("\nConvergence Diagnostics:")
        print(f"R-hat: {self.diagnostics['r_hat']}")
        print(f"Effective Sample Size: {self.diagnostics['ess']}")
        print(f"Integrated ACT: {self.diagnostics['act']}")

        print("\nSampling Statistics:")
        print(f"Final acceptance rate: {self.diagnostics['acceptance_rate']:.3f}")
        print(f"Number of samples: {self.diagnostics['n_samples']}")

        print("\nParameter Estimates:")
        print("Means:")
        print(f"Estimated: {self.diagnostics['mean_estimate']}")
        print("\nCovariance:")
        print(self.diagnostics["covariance_estimate"])

    def plot_target_distribution(self):
        """Plot an approximation of the target distribution with KDE and MCMC samples"""
        # Extract chain samples
        x_samples, y_samples = self.chain[:, 0], self.chain[:, 1]

        # Compute KDE on the samples
        kde = gaussian_kde(self.chain.T)

        # Determine plot ranges with padding
        x_min, x_max = x_samples.min(), x_samples.max()
        y_min, y_max = y_samples.min(), y_samples.max()
        x_pad = 0.1 * (x_max - x_min)
        y_pad = 0.1 * (y_max - y_min)

        # Create grid for plotting
        x = np.linspace(x_min - x_pad, x_max + x_pad, 500)
        y = np.linspace(y_min - y_pad, y_max + y_pad, 500)
        x1, x2 = np.meshgrid(x, y)
        pos = np.vstack([x1.ravel(), x2.ravel()])

        # Evaluate KDE on the grid
        Z = kde(pos).reshape(x1.shape)

        # Plot KDE contours
        plt.figure(figsize=(10, 8))
        plt.contourf(x1, x2, Z, levels=30, cmap="viridis", alpha=0.8)
        plt.colorbar(label="Density")

        # Overlay MCMC samples
        plt.scatter(
            x_samples, y_samples, alpha=0.1, color="k", s=1, label="MCMC samples"
        )

        ax = plt.gca()
        ax.set_aspect("equal", adjustable="box")

        plt.xlabel("X1")
        plt.ylabel("X2")
        plt.title("Target Distribution Approximation with MCMC Samples")
        plt.legend()
        plt.show()

================
File: Distributions.py
================
from typing import Tuple, Union

import numpy as np
import scipy as sp

from LibMCMC.PRNG import RNG, SEED

## TODO: I need to Add Adaptation MCMC Diagnostics As well.
#    # Plot 5: Adaptation factor
#    adapt_factors = []
#   for i in range(len(chain)):
#       sampler._index = i
#       adapt_factors.append(sampler.get_adaptation_weight())
#   axes[1, 1].plot(adapt_factors)
#   axes[1, 1].set_xlabel("Iteration")
#   axes[1, 1].set_ylabel("Adaptation Weight")
#   axes[1, 1].set_title("Adaptation Weight Decay")


class Proposal:
    def __init__(self, proposal_distribution: sp.stats.rv_continuous, scale):
        self.proposal_distribution = proposal_distribution
        self.proposal = RNG(SEED // 2, proposal_distribution)
        if np.isscalar(scale):
            print("scalar")
            self.beta = np.sqrt(scale)
        else:
            self.beta = scale

    #            print("Cholesky")
    #            self.beta = sp.stats.Covariance.from_cholesky(scale)  # L*x ~ N(0, Sigma)

    def propose(self, current: np.ndarray):
        return self.proposal(current, self.beta)

    def proposal_log_density(
        self,
        state: np.ndarray,
        loc: np.ndarray,
    ) -> np.float64:
        return self.proposal_distribution.logpdf(state, loc, self.beta)


# Test Proposal
# test = Proposal(sp.stats.multivariate_normal, np.array([[1, 2], [2, 1]]))
# print(test.propose(np.array([1.0, 12])))
# print(test.proposal_log_density(np.array([1.0, 12]), np.array([1.0, 12])))


class TargetDistribution:
    def __init__(
        self,
        prior: sp.stats.rv_continuous,
        likelihood: sp.stats.rv_continuous,
        data,
        sigma: float,
    ):
        self.prior = prior
        self.likelihood = likelihood
        # likelihood
        self.data = data
        self.data_sigma = sigma

    def log_likelihood(self, x: np.ndarray) -> np.float64:
        """
        Likelihood of our data given the parameters x.
        I.E the distribution of the data given the parameters x.
        :param x:
        :return:
        """
        return np.sum(self.likelihood.logpdf(self.data, x, self.data_sigma))

    def log_prior(self, x: np.ndarray) -> np.float64:
        if x.ndim == 2 and x.shape[1] == 1:
            x = x.reshape(-1, 1)

        if not hasattr(self.prior, "mean"):  # Quick way to check if it's non-frozen
            return self.prior.logpdf(x[np.newaxis, :])

        return self.prior.logpdf(x)


class BayesInverseGammaVarianceDistribution(TargetDistribution):
    """Target distribution with inverse gamma prior on noise variance"""

    def __init__(
        self,
        prior: sp.stats.rv_continuous,
        likelihood: sp.stats.rv_continuous,
        data,
        alpha: float = 2.0,  # Shape parameter for inverse gamma
        beta: float = 1.0,  # Scale parameter for inverse gamma
    ):
        # Initialize parent without sigma since we're marginalizing it
        super().__init__(prior, likelihood, data, sigma=None)

        # Store inverse gamma hyperparameters
        self.alpha = alpha
        self.beta = beta

    def log_likelihood(self, x: np.ndarray) -> np.float64:
        """
        Compute marginalized log likelihood integrating out σ²
        p(y|θ) = ∫ p(y|θ,σ²)p(σ²)dσ²
        """
        residuals = self.data - x  # Or self.forward_model(x) for complex models
        n = len(residuals)
        RSS = np.sum(residuals**2)

        # Updated inverse gamma parameters
        alpha_post = self.alpha + n / 2
        beta_post = self.beta + RSS / 2

        # Log marginal likelihood (multivariate t-distribution)
        log_lik = -alpha_post * np.log(beta_post)
        log_lik += sp.special.gammaln(alpha_post) - sp.special.gammaln(self.alpha)
        log_lik -= (n / 2) * np.log(2 * np.pi)

        return np.float64(log_lik)

    def sample_variance_posterior(self, x: np.ndarray) -> float:
        """Sample from conditional posterior of σ² given parameters"""
        residuals = self.data - x
        n = len(residuals)
        RSS = np.sum(residuals**2)

        # Posterior inverse gamma parameters
        alpha_post = self.alpha + n / 2
        beta_post = self.beta + RSS / 2

        return sp.stats.invgamma.rvs(alpha_post, scale=beta_post)

================
File: GradientUtils.py
================
from typing import Callable, Optional

import numpy as np
import scipy as sp
import torch
from scipy.optimize import approx_fprime

from LibMCMC.Distributions import TargetDistribution


class GradientCalculator:
    """Utilities for computing gradients of log target densities"""

    def __init__(self, target: TargetDistribution, epsilon: float = 1e-8):
        """
        Initialize gradient calculator.

        Args:
            target: Target distribution object
            epsilon: Step size for finite difference approximation
        """
        self.target = target
        self.epsilon = epsilon

    def log_target_density(self, x: np.ndarray) -> np.float64:
        """Compute log target density (log prior + log likelihood)"""
        return self.target.log_prior(x) + self.target.log_likelihood(x)

    def numerical_gradient(self, x: np.ndarray) -> np.ndarray:
        """
        Compute gradient using finite differences.
        Uses scipy.optimize.approx_fprime for accurate approximation.
        """
        return approx_fprime(x, self.log_target_density, self.epsilon)

    def numerical_gradient_custom(self, x: np.ndarray) -> np.ndarray:
        """
        Alternative implementation using central differences.
        More accurate but slower than forward differences.
        """
        grad = np.zeros_like(x)
        for i in range(len(x)):
            h = np.zeros_like(x)
            h[i] = self.epsilon
            grad[i] = (
                self.log_target_density(x + h) - self.log_target_density(x - h)
            ) / (2 * self.epsilon)
        return grad

    @staticmethod
    def create_scipy_gradient(dist: sp.stats.rv_continuous) -> Callable:
        """
        Create gradient function for scipy distributions that have
        built-in score functions.

        Args:
            dist: scipy.stats distribution object

        Returns:
            Gradient function for the log pdf
        """
        if hasattr(dist, "score"):
            return lambda x: dist.score(x)
        else:
            raise NotImplementedError(
                f"Distribution {type(dist)} does not have a score function"
            )


class MALAGradient:
    def __init__(self, target: TargetDistribution):
        self.target = target

    def __call__(self, x: np.ndarray) -> np.ndarray:
        """Compute gradient using vectorized central differences"""
        epsilon = 1e-8
        x = x.astype(np.float64)
        dim = len(x)

        # Create perturbation matrix
        h_mat = np.eye(dim) * epsilon

        # Compute forward differences vectorized
        f_plus = np.array(
            [
                self.target.log_prior(x + h_mat[i])
                + self.target.log_likelihood(x + h_mat[i])
                for i in range(dim)
            ]
        )

        # Compute backward differences vectorized
        f_minus = np.array(
            [
                self.target.log_prior(x - h_mat[i])
                + self.target.log_likelihood(x - h_mat[i])
                for i in range(dim)
            ]
        )

        # Central difference approximation
        grad = (f_plus - f_minus) / (2 * epsilon)
        return grad


def get_gradient_function(
    target: TargetDistribution, method: str = "auto", epsilon: float = 1e-8
) -> Callable:
    """
    Factory function to create gradient function for multidimensional distributions.
    """

    def log_target_density(x: np.ndarray) -> np.float64:
        return target.log_prior(x) + target.log_likelihood(x)

    if method == "auto":
        return MALAGradient(target)
    if method == "numerical":

        def numerical_gradient(x: np.ndarray) -> np.ndarray:
            # Compute gradient using finite differences for each dimension
            grad = np.zeros_like(x)

            # For each dimension, compute partial derivative
            for i in range(len(x)):
                h = np.zeros_like(x)
                h[i] = epsilon  # Perturbation in i-th dimension

                # Central difference approximation
                # (f(x + h) - f(x - h)) / (2h)
                grad[i] = (log_target_density(x + h) - log_target_density(x - h)) / (
                    2 * epsilon
                )

            return grad

        return numerical_gradient
    else:
        raise ValueError(f"Unknown gradient method: {method}")

================
File: MALA.py
================
## Metropolis Adjusted Langevin Algorithm
import time
from typing import Callable, Dict, Optional

import numpy as np
import scipy as sp

from LibMCMC.Diagnostics import MCMCDiagnostics
from LibMCMC.Distributions import Proposal, TargetDistribution
from LibMCMC.GradientUtils import get_gradient_function
from LibMCMC.MetropolisHastings import MetropolisHastings


class MALAProposal(Proposal):
    """MALA-specific proposal distribution incorporating gradient information"""

    def __init__(
        self,
        proposal_distribution: sp.stats.rv_continuous,
        scale: float,
        gradient_func: Callable,
    ):
        """
        Initialize MALA proposal.

        Args:
            proposal_distribution: Base proposal distribution (should be normal)
            scale: Initial step size (epsilon) for the Langevin dynamics
            gradient_func: Function computing gradient of log target density
        """
        super().__init__(proposal_distribution, 1.0)  # Initialize base with unit scale
        self.gradient_func = gradient_func
        self.epsilon = scale  # Step size for Langevin dynamics

    def propose(self, current: np.ndarray) -> np.ndarray:
        """
        Generate proposal using Langevin dynamics:
        x* = x + (ε/2)∇log(π(x)) + √ε * z, where z ~ N(0,I)
        """
        grad_log_target = self.gradient_func(current)
        drift = 0.5 * self.epsilon * grad_log_target
        # Use the RNG class's __call__ method instead of rvs
        diffusion = np.sqrt(self.epsilon) * self.proposal(0, 1, size=len(current))
        return current + drift + diffusion

    def proposal_log_density(
        self, proposed: np.ndarray, current: np.ndarray
    ) -> np.float64:
        """
        Compute log q(x'|x) for the MALA proposal.
        This is Gaussian with mean μ(x) = x + (ε/2)∇log(π(x))
        and variance ε*I
        """
        grad_log_target = self.gradient_func(current)
        mean = current + 0.5 * self.epsilon * grad_log_target
        return np.sum(
            sp.stats.norm.logpdf(proposed, loc=mean, scale=np.sqrt(self.epsilon))
        )


class MALA(MetropolisHastings):
    """Metropolis Adjusted Langevin Algorithm implementation"""

    def __init__(
        self,
        target_distribution: TargetDistribution,
        initial_state: np.ndarray,
        step_size: float = 0.1,
        adaptation_interval: int = 50,
        gradient_method: str = "numerical",
    ):
        """
        Initialize MALA sampler.

        Args:
            target_distribution: Target distribution to sample from
            gradient_func: Function computing gradient of log target density
            initial_state: Starting point for the chain
            step_size: Initial Langevin step size (epsilon)
            adaptation_interval: How often to adapt step size
        """
        # Get gradient function
        gradient_func = get_gradient_function(target_distribution, gradient_method)

        # Initialize MALA-specific proposal
        proposal = MALAProposal(
            sp.stats.norm,  # Always use normal for MALA
            step_size,
            gradient_func,
        )

        super().__init__(target_distribution, proposal, initial_state)

        self.gradient_func = gradient_func
        self.adaptation_interval = adaptation_interval

    def adapt_step_size(self):
        """
        Adapt the step size to achieve optimal acceptance rate (0.57 for MALA).
        Uses Robbins-Monro stochastic approximation.
        """
        if self._index < 100:  # Wait for some samples
            return

        current_acceptance = self.acceptance_count / self._index
        target_acceptance = 0.57  # Optimal acceptance rate for MALA

        # Robbins-Monro update
        factor = min(0.01, 1.0 / np.sqrt(self._index))
        log_step_size = np.log(self.proposal_distribution.epsilon)
        log_step_size += factor * (current_acceptance - target_acceptance)

        self.proposal_distribution.epsilon = np.exp(log_step_size)

    def __call__(self, n: int):
        """Run MALA for n iterations with step size adaptation"""
        if self._index + n > self.max_size:
            new_max = max(self.max_size * 2, self._index + n)
            new_chain = np.empty((new_max, self.chain.shape[1]))
            new_chain[: self._index] = self.chain[: self._index]
            self.chain = new_chain
            new_acceptance_rates = np.empty(new_max)
            new_acceptance_rates[: self._index] = self.acceptance_rates[: self._index]
            self.acceptance_rates = new_acceptance_rates
            self.max_size = new_max

        for i in range(n):
            if i == 200:
                self.burn(199)

            current = self.chain[self._index - 1]
            proposed = self.proposal_distribution.propose(current)

            log_alpha = self.acceptance_ratio(current, proposed)

            if np.log(self.uniform_rng.uniform()) < log_alpha:
                self.chain[self._index] = proposed
                self.acceptance_count += 1
            else:
                self.chain[self._index] = current

            if self._index % self.adaptation_interval == 0:
                self.adapt_step_size()

            self.acceptance_rates[self._index] = self.acceptance_count / self._index
            self._index += 1


class AdaptiveMALA(MALA):
    """MALA with preconditioning matrix adaptation"""

    def __init__(
        self,
        target_distribution: TargetDistribution,
        gradient_func: Callable,
        initial_state: np.ndarray,
        step_size: float = 0.1,
        adaptation_interval: int = 50,
        min_samples_adapt: int = 500,
        max_samples_adapt: int = 1000,
    ):
        super().__init__(
            target_distribution,
            gradient_func,
            initial_state,
            step_size,
            adaptation_interval,
        )

        self.min_samples_adapt = min_samples_adapt
        self.max_samples_adapt = max_samples_adapt

        # Initialize preconditioning matrix estimate
        d = len(initial_state)
        self.empirical_mean = initial_state.copy()
        self.preconditioner = np.eye(d)

    def update_preconditioner(self, new_sample: np.ndarray):
        """Update preconditioning matrix using sample covariance"""
        n = self._index - self.min_samples_adapt
        if n <= 0:
            return

        old_mean = self.empirical_mean.copy()
        self.empirical_mean = (n * old_mean + new_sample) / (n + 1)

        # Update sample covariance
        cov_update = (n / (n + 1)) * self.preconditioner + (
            n / ((n + 1) ** 2)
        ) * np.outer(old_mean - new_sample, old_mean - new_sample)

        # Ensure numerical stability
        self.preconditioner = cov_update + 1e-6 * np.eye(len(new_sample))

    def __call__(self, n: int):
        """Run preconditioned MALA"""
        if self._index + n > self.max_size:
            new_max = max(self.max_size * 2, self._index + n)
            new_chain = np.empty((new_max, self.chain.shape[1]))
            new_chain[: self._index] = self.chain[: self._index]
            self.chain = new_chain
            new_acceptance_rates = np.empty(new_max)
            new_acceptance_rates[: self._index] = self.acceptance_rates[: self._index]
            self.acceptance_rates = new_acceptance_rates
            self.max_size = new_max

        for i in range(n):
            if i == 200:
                self.burn(199)

            current = self.chain[self._index - 1]
            proposed = self.proposal_distribution.propose(current)

            log_alpha = self.acceptance_ratio(current, proposed)

            if np.log(self.uniform_rng.uniform()) < log_alpha:
                accepted_state = proposed
                self.chain[self._index] = proposed
                self.acceptance_count += 1
            else:
                accepted_state = current
                self.chain[self._index] = current

            if self._index % self.adaptation_interval == 0:
                self.adapt_step_size()
                if self._index >= self.min_samples_adapt:
                    self.update_preconditioner(accepted_state)

            self.acceptance_rates[self._index] = self.acceptance_count / self._index
            self._index += 1


class GaussianTarget(TargetDistribution):
    def __init__(self):
        prior = sp.stats.norm
        likelihood = sp.stats.norm
        # 2D target - generating two parameters
        data = sp.stats.norm.rvs(1, 1, 1000)

        super().__init__(prior, likelihood, data, 1.0)


# Example usage:
if __name__ == "__main__":
    target = GaussianTarget()
    initial_state = np.array([0.0])

    # Create MALA sampler
    sampler = MALA(
        target_distribution=target,
        initial_state=initial_state,
        step_size=0.0030,
        gradient_method="numerical",
    )

    sampler(10000)
    proposed = sampler.proposal_distribution.propose(initial_state)
    print(f"Proposed state: {proposed}")
    print(f"Acceptance ratio: {sampler.acceptance_ratio(initial_state, proposed)}")
    diagnostics = MCMCDiagnostics(sampler, true_value=target.data)

    diagnostics.print_summary()

================
File: MetropolisHastings.py
================
import numpy as np
import scipy as sp

from LibMCMC.Distributions import Proposal, TargetDistribution
from LibMCMC.PRNG import SEED


class MetropolisHastings:
    def __init__(
        self,
        target_distribution: TargetDistribution,
        proposal_distribution: Proposal,
        initialstate,
    ):
        """
        Initialize the Metropolis-Hastings algorithm.
        :param target_distribution:
        :param proposal_distribution:
        :param initialstate:
        """
        self.target_distribution = target_distribution
        self.proposal_distribution = proposal_distribution

        self.max_size = 10000  # Or some reasonable default
        self.chain = np.zeros((self.max_size, len(initialstate)))
        self.chain[0] = initialstate
        self._index = 1
        self.uniform_rng = np.random.default_rng(
            seed=SEED // 3
        )  # Using Philox for Reproducability

        self.acceptance_count = 0
        self.acceptance_rates = np.zeros(self.max_size)

        self.burnt = False

    def acceptance_ratio(self, current: np.ndarray, proposed: np.ndarray) -> np.float64:
        """
        Acceptance ratio for the Metropolis-Hastings algorithm.
        :param current: Current state of the chain.
        :param proposed: Proposed state of the chain.
        :return: Acceptance ratio.
        """

        prior_ratio = self.target_distribution.log_prior(
            proposed
        ) - self.target_distribution.log_prior(current)
        likelihood_ratio = self.target_distribution.log_likelihood(
            proposed
        ) - self.target_distribution.log_likelihood(current)
        transition_ratio = self.proposal_distribution.proposal_log_density(
            current, proposed
        ) - self.proposal_distribution.proposal_log_density(
            proposed, current
        )  # Previous given new over new given previous
        #        assert np.isscalar(prior_ratio)
        #        assert np.isscalar(likelihood_ratio)
        #        assert np.isscalar(transition_ratio)
        #
        log_ratio = prior_ratio + likelihood_ratio + transition_ratio
        return min(np.float64(0), log_ratio)

    def __call__(self, n: int):
        """
        Run the Metropolis-Hastings algorithm for n iterations.
        :param n:
        """
        # Resize if needed
        if self._index + n > self.max_size:
            new_max = max(self.max_size * 2, self._index + n)
            new_chain = np.empty((new_max, self.chain.shape[1]))
            new_chain[: self._index] = self.chain[: self._index]
            self.chain = new_chain
            self.max_size = new_max
            new_acceptance_rates = np.empty(new_max)
            new_acceptance_rates[: self._index] = self.acceptance_rates[: self._index]
            self.acceptance_rates = new_acceptance_rates

        for i in range(n):
            if i == 200:
                self.burn(199)
            current = self.chain[self._index - 1]
            proposed = self.proposal_distribution.propose(current)
            if np.log(self.uniform_rng.uniform()) < self.acceptance_ratio(
                current, proposed
            ):
                self.chain[self._index] = proposed
                self.acceptance_count += 1
            else:
                self.chain[self._index] = current

            self.acceptance_rates[self._index] = self.acceptance_count / self._index
            self._index += 1

    def burn(self, n: int):
        if n < self._index:
            # Keep only samples after burn point
            burned_chain = self.chain[n : self._index]

            # Create new arrays and copy burned data
            new_chain = np.empty((self.max_size, self.chain.shape[1]))
            new_chain[: len(burned_chain)] = burned_chain

            # Reset acceptance tracking
            self.acceptance_count = 0
            self.acceptance_rates = np.zeros(self.max_size)

            # Update chain and index
            self.chain = new_chain
            self._index = len(burned_chain)
            print("burnt is: ", self.burnt)
            self.burnt = True

        else:
            raise ValueError("Burn-in exceeds the number of samples in the chain.")

    #    def burn(self, n):
    #        print("chain delte: ", len(self.chain))
    #        self.chain = self.chain[n:]
    #       print("chain deleteafter: ", len(self.chain))

    def writeToFile(self, filename: str) -> None:
        """
        Write the MCMC chain to a CSV file.

        Args:
            filename: Path to the CSV file
        """
        import numpy as np

        # Get the actual chain data (exclude empty rows)
        chain_data = self.chain[: self._index]

        # Write using numpy's savetxt
        np.savetxt(filename, chain_data, delimiter=",", fmt="%.8f")


class AdaptiveMetropolisHastingsVOne(MetropolisHastings):
    def __init__(
        self,
        target: TargetDistribution,
        proposal: Proposal,
        initial_value: np.ndarray,
        adaptation_interval: int = 50,
        min_samples_adapt: int = 500,
        max_samples_adapt: int = 1000,
    ):
        # Use parent class initialization
        super().__init__(target, proposal, initial_value)

        # Add only what's needed for adaptation
        self.adaptation_interval = adaptation_interval
        self.min_samples_adapt = min_samples_adapt
        self.max_samples_adapt = max_samples_adapt

        # Setup for covariance adaptation
        d = len(initial_value)
        self.scale = (2.38**2) / d
        self.all_samples = np.zeros((max_samples_adapt, d))
        self.all_samples[0] = initial_value
        self.n_samples = 1

    def update_covariance(self, state: np.ndarray):
        """Update covariance estimate with new state"""
        if self.n_samples >= self.max_samples_adapt:
            return

        self.all_samples[self.n_samples] = state

        if self.n_samples > 1:
            # Update proposal covariance using all samples
            sample_cov = np.cov(self.all_samples[: self.n_samples + 1].T)
            reg = 1e-6 * np.diag(np.diag(sample_cov))
            self.proposal_distribution.beta = self.scale * (sample_cov + reg)

        self.n_samples += 1

    def __call__(self, n: int):
        # Use parent's chain management
        if self._index + n > self.max_size:
            new_max = max(self.max_size * 2, self._index + n)
            new_chain = np.empty((new_max, self.chain.shape[1]))
            new_chain[: self._index] = self.chain[: self._index]
            self.chain = new_chain
            new_acceptance_rates = np.empty(new_max)
            new_acceptance_rates[: self._index] = self.acceptance_rates[: self._index]
            self.acceptance_rates = new_acceptance_rates
            self.max_size = new_max

        for i in range(n):
            if i == 200:
                self.burn(199)

            current = self.chain[self._index - 1]
            proposed = self.proposal_distribution.propose(current)

            # Use parent's acceptance ratio
            log_alpha = self.acceptance_ratio(current, proposed)

            if np.log(self.uniform_rng.uniform()) < log_alpha:
                self.chain[self._index] = proposed
                self.acceptance_count += 1
            else:
                self.chain[self._index] = current

            # Update covariance using current state
            if self._index % self.adaptation_interval == 0:
                self.update_covariance(self.chain[self._index])

            self.acceptance_rates[self._index] = self.acceptance_count / self._index
            self._index += 1


class AdaptiveMetropolisHastings(MetropolisHastings):
    def __init__(
        self,
        target: TargetDistribution,
        proposal: Proposal,
        initial_value: np.ndarray,
        adaptation_interval: int = 1,  # Changed to 1 based on Haario et al.
        target_acceptance: float = 0.234,  # Roberts & Rosenthal optimal
        adaptation_scale: float = 2.4,  # Haario et al. optimal
        min_samples_adapt: int = 800,  # Earlier adaptation from Roberts & Rosenthal
        max_samples_adapt: int = 1500,
    ):
        super().__init__(target, proposal, initial_value)
        self.adaptation_interval = adaptation_interval
        self.target_acceptance = target_acceptance

        # Optimal scaling from Haario et al.
        d = len(initial_value)
        self.adaptation_scale = (adaptation_scale**2) / d

        self.min_samples_adapt = min_samples_adapt
        self.max_samples_adapt = max_samples_adapt

        # Initialize empirical mean and covariance (Haario et al.)
        self.empirical_mean = initial_value.copy()
        self.empirical_cov = np.eye(d)
        self.n_samples = 1

    def update_empirical_estimates(self, new_sample: np.ndarray):
        """Update running covariance estimate using R&R 2009 method"""
        n = self._index - self.min_samples_adapt
        if n <= 0:
            return

        # Update mean
        old_mean = self.empirical_mean.copy()
        self.empirical_mean = (n * old_mean + new_sample) / (n + 1)

        # Update covariance using R&R formula
        self.empirical_cov = (n / (n + 1)) * self.empirical_cov + (
            n / ((n + 1) ** 2)
        ) * np.outer(old_mean - new_sample, old_mean - new_sample)

    def get_adaptation_weight(self) -> float:
        """Roberts & Rosenthal 2009 adaptation rate"""
        if self._index <= self.min_samples_adapt:
            return 1.0
        elif self._index >= self.max_samples_adapt:
            return 0.0

        t = self._index - self.min_samples_adapt
        # This gives rate decaying like 1/t
        return min(1.0, self.adaptation_scale / t)

    def update_proposal(self):
        """Update proposal covariance using R&R scheme"""
        if self._index < self.min_samples_adapt:
            return
        if self._index == self.min_samples_adapt:
            print(self.proposal_distribution.beta)
            return

        if self._index > self.max_samples_adapt:
            return
        if self._index == self.max_samples_adapt:
            print(self.proposal_distribution.beta)
            return

        scaled_cov = self.empirical_cov
        scaled_cov += 1e-6 * np.eye(self.empirical_cov.shape[0])

        self.proposal_distribution.beta = scaled_cov

    def __call__(self, n: int):
        """Run adaptive MCMC with online updates"""
        if self._index + n > self.max_size:
            new_max = max(self.max_size * 2, self._index + n)
            new_chain = np.empty((new_max, self.chain.shape[1]))
            new_chain[: self._index] = self.chain[: self._index]
            self.chain = new_chain
            self.max_size = new_max
            new_acceptance_rates = np.empty(new_max)
            new_acceptance_rates[: self._index] = self.acceptance_rates[: self._index]
            self.acceptance_rates = new_acceptance_rates
            self.max_size = new_max

        for i in range(n):
            if (self._index == self.min_samples_adapt // 2) and not self.burnt:
                print("Burning")
                self.burn(self.min_samples_adapt // 2 - 1)
            current = self.chain[self._index - 1]
            proposed = self.proposal_distribution.propose(current)

            log_alpha = self.acceptance_ratio(current, proposed)

            if np.log(self.uniform_rng.uniform()) < log_alpha:
                accepted_state = proposed
                self.chain[self._index] = proposed
                self.acceptance_count += 1
                # Update empirical estimates only on acceptance
            else:
                accepted_state = current
                self.chain[self._index] = current

            if (
                self._index % self.adaptation_interval == 0
                and self._index > self.min_samples_adapt
            ):
                self.update_empirical_estimates(accepted_state)
                self.update_proposal()

            self.acceptance_rates[self._index] = self.acceptance_count / self._index
            self._index += 1

================
File: PRNG.py
================
from numpy.random import SeedSequence, Generator, Philox
from scipy.stats import rv_continuous
from typing import Optional

# Set Global Seed Here
SEED = 1000


# Parent RNG class
class RNG:
    def __init__(self, seed: Optional[int], distribution: rv_continuous):
        self.ss = SeedSequence(seed)
        self.rg = Generator(Philox(self.ss))  # Use Philox for parallel applications
        self.rng_distribution = distribution

    def __call__(self, loc, scale, size: Optional[int] = None, *args, **kwargs):
        return self.rng_distribution.rvs(
            loc, scale, size=size, random_state=self.rg, *args, **kwargs
        )
